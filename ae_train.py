"""
================================================================================
AEON TRAINING PIPELINE v4.0 - CONNECTED THOUGHTS EDITION
================================================================================

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è v4.0:
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π RSSM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º
- ‚úÖ –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (grad_clip —Å–Ω–∏–∂–µ–Ω –¥–æ 0.5)
- ‚úÖ Entropy regularization –¥–ª—è –∫–æ–¥–±—É–∫–∞
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ z_pairs
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π warmup –∏ scheduling

–ê–≤—Ç–æ—Ä: AEON Research Team
–í–µ—Ä—Å–∏—è: 4.0.0
"""

import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import List, Tuple, Dict, Optional, Union, Any
from dataclasses import dataclass, field, asdict
import math
from tqdm import tqdm
import logging
import os
import time
from datetime import datetime, timedelta
from torch.utils.data import DataLoader, TensorDataset, Dataset
import argparse
import copy
import warnings

warnings.filterwarnings('ignore', category=UserWarning)

# --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---
try:
    from transformers import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.")

# --- Mixed Precision ---
try:
    from torch.amp import GradScaler, autocast
    AMP_AVAILABLE = torch.cuda.is_available()
except ImportError:
    try:
        from torch.cuda.amp import GradScaler, autocast
        AMP_AVAILABLE = torch.cuda.is_available()
    except ImportError:
        AMP_AVAILABLE = False


# ==============================================================================
# –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
# ==============================================================================

def configure_logger(logfile: Optional[str] = None, level=logging.INFO) -> logging.Logger:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω—Å–æ–ª–∏"""
    logger = logging.getLogger("AEON-Training-v4")
    logger.setLevel(level)
    logger.handlers.clear()
    
    detailed_format = '%(asctime)s | %(levelname)-8s | %(message)s'
    
    sh = logging.StreamHandler()
    sh.setFormatter(logging.Formatter(detailed_format))
    logger.addHandler(sh)
    
    if logfile:
        log_dir = os.path.dirname(logfile) or "."
        os.makedirs(log_dir, exist_ok=True)
        fh = logging.FileHandler(logfile, encoding="utf-8")
        fh.setFormatter(logging.Formatter(detailed_format))
        logger.addHandler(fh)
    
    return logger

logger = configure_logger()


# ==============================================================================
# –£–°–¢–†–û–ô–°–¢–í–û
# ==============================================================================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"üñ•Ô∏è  Device: {device}")
if torch.cuda.is_available():
    logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")


# ==============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø v4.0 ‚Äî –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –î–õ–Ø –°–í–Ø–ó–ê–ù–ù–´–• –ú–´–°–õ–ï–ô
# ==============================================================================

@dataclass
class AEONConfigV4:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v4.0 —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π
    
    –ö–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
    - grad_clip_norm: 0.5 (–±—ã–ª–æ 1.0) ‚Äî —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è
    - context_window: 3 ‚Äî RSSM —É—á–∏—Ç—ã–≤–∞–µ—Ç 3 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è
    - entropy_weight: 0.1 ‚Äî —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–æ–¥–±—É–∫–∞
    - document_aware: True ‚Äî –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–∞—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    """
    
    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    z_dim: int = 256
    hidden_dim: int = 256
    vocab_size: int = 30522
    num_pillars: int = 5
    seq_length: int = 64
    
    # VQ-VAE (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
    vq_num_embeddings: int = 2048
    vq_embedding_dim: int = 256
    vq_commitment_cost: float = 0.25
    vq_loss_weight: float = 0.5
    vq_ema_decay: float = 0.99
    vq_temperature: float = 1.0
    vq_reset_threshold: int = 30  # –ë—ã–ª–æ 50, —Ç–µ–ø–µ—Ä—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ
    
    # ‚úÖ –ù–û–í–û–ï: Entropy regularization
    entropy_weight: float = 0.1  # –ü–æ–æ—â—Ä—è–µ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤
    
    # –û–±—É—á–µ–Ω–∏–µ (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
    learning_rate: float = 3e-5
    min_learning_rate: float = 1e-6
    weight_decay: float = 0.01
    grad_clip_norm: float = 0.5  # ‚úÖ –ë—ã–ª–æ 1.0, —Ç–µ–ø–µ—Ä—å —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
    batch_size: int = 16
    gradient_accumulation_steps: int = 2
    
    # Warmup –∏ Scheduling
    warmup_steps: int = 1000  # –ë—ã–ª–æ 500, —Ç–µ–ø–µ—Ä—å –ø–ª–∞–≤–Ω–µ–µ
    warmup_ratio: float = 0.1
    
    # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    dropout_rate: float = 0.1
    label_smoothing: float = 0.1
    
    # ‚úÖ –ù–û–í–û–ï: RSSM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    context_window: int = 3  # RSSM –≤–∏–¥–∏—Ç 3 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö z
    rssm_hidden_dim: int = 512  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    # ‚úÖ –ù–û–í–û–ï: –î–æ–∫—É–º–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    document_aware: bool = True  # –°—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä—ã —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    min_doc_chunks: int = 2  # –ú–∏–Ω–∏–º—É–º —á–∞–Ω–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    
    # Early Stopping
    early_stopping_patience: int = 5
    min_delta: float = 1e-4
    
    # Checkpointing
    save_every_n_epochs: int = 5
    keep_n_checkpoints: int = 3
    
    # –ü—Ä–æ—á–µ–µ
    seed: int = 42
    use_amp: bool = True
    
    # Noise scale for VQ code reset
    code_reset_noise_scale: float = 0.05

    def __post_init__(self):
        """Validate configuration parameters."""
        if self.z_dim <= 0:
            raise ValueError(f"z_dim must be positive, got {self.z_dim}")
        if self.hidden_dim <= 0:
            raise ValueError(f"hidden_dim must be positive, got {self.hidden_dim}")
        if self.vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {self.vocab_size}")
        if self.seq_length <= 0:
            raise ValueError(f"seq_length must be positive, got {self.seq_length}")
        if self.batch_size <= 0:
            raise ValueError(f"batch_size must be positive, got {self.batch_size}")
        if self.gradient_accumulation_steps <= 0:
            raise ValueError(f"gradient_accumulation_steps must be positive, got {self.gradient_accumulation_steps}")
        if self.learning_rate <= 0:
            raise ValueError(f"learning_rate must be positive, got {self.learning_rate}")
        if self.min_learning_rate <= 0:
            raise ValueError(f"min_learning_rate must be positive, got {self.min_learning_rate}")
        if self.vq_commitment_cost < 0:
            raise ValueError(f"vq_commitment_cost must be non-negative, got {self.vq_commitment_cost}")
        if self.context_window < 1:
            raise ValueError(f"context_window must be >= 1, got {self.context_window}")
        if self.vq_num_embeddings < 2:
            raise ValueError(f"vq_num_embeddings must be >= 2, got {self.vq_num_embeddings}")
        if not (0 < self.vq_ema_decay < 1):
            raise ValueError(f"vq_ema_decay must be in (0, 1), got {self.vq_ema_decay}")
        if self.code_reset_noise_scale < 0:
            raise ValueError(f"code_reset_noise_scale must be non-negative, got {self.code_reset_noise_scale}")
        if not (0 <= self.warmup_ratio <= 1):
            raise ValueError(f"warmup_ratio must be in [0, 1], got {self.warmup_ratio}")
        if not (0 <= self.dropout_rate < 1):
            raise ValueError(f"dropout_rate must be in [0, 1), got {self.dropout_rate}")
        if not (0 <= self.label_smoothing < 1):
            raise ValueError(f"label_smoothing must be in [0, 1), got {self.label_smoothing}")
        if self.vq_temperature <= 0:
            raise ValueError(f"vq_temperature must be positive, got {self.vq_temperature}")
        if self.entropy_weight < 0:
            raise ValueError(f"entropy_weight must be non-negative, got {self.entropy_weight}")
        if self.vq_loss_weight < 0:
            raise ValueError(f"vq_loss_weight must be non-negative, got {self.vq_loss_weight}")
        if self.save_every_n_epochs <= 0:
            raise ValueError(f"save_every_n_epochs must be positive, got {self.save_every_n_epochs}")
        if self.keep_n_checkpoints <= 0:
            raise ValueError(f"keep_n_checkpoints must be positive, got {self.keep_n_checkpoints}")
        if self.min_doc_chunks < 1:
            raise ValueError(f"min_doc_chunks must be >= 1, got {self.min_doc_chunks}")


# ==============================================================================
# –ú–û–ù–ò–¢–û–† –û–ë–£–ß–ï–ù–ò–Ø (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
# ==============================================================================

class TrainingMonitor:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, logger: logging.Logger, save_dir: str = "checkpoints"):
        self.logger = logger
        self.metrics_history = {"phase_A": [], "phase_B": []}
        self.batch_metrics = {"phase_A": [], "phase_B": []}
        self.start_time = None
        self.epoch_start_time = None
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        self.best_loss = float('inf')
        self.patience_counter = 0
        
    def start_training(self, phase: str, total_epochs: int, total_samples: int):
        self.start_time = time.time()
        self.batch_metrics[phase] = []
        self.logger.info("=" * 75)
        self.logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø - {phase}")
        self.logger.info(f"   –í—Å–µ–≥–æ —ç–ø–æ—Ö: {total_epochs}")
        self.logger.info(f"   –í—Å–µ–≥–æ —Å—ç–º–ø–ª–æ–≤: {total_samples:,}")
        self.logger.info(f"   –í—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 75)
        
    def start_epoch(self, epoch: int, total_epochs: int):
        self.epoch_start_time = time.time()
        self.logger.info(f"\n{'‚îÄ' * 60}")
        self.logger.info(f"üìç –≠–ø–æ—Ö–∞ {epoch + 1}/{total_epochs}")
        self.logger.info(f"{'‚îÄ' * 60}")
        
    def log_batch(self, batch_idx: int, total_batches: int, metrics: dict, 
                  phase: str = "phase_A", log_every: int = 10):
        self.batch_metrics[phase].append(metrics.copy())
        
        if batch_idx % log_every == 0 or batch_idx == total_batches - 1:
            metrics_str = " | ".join([f"{k}: {v:.6f}" for k, v in metrics.items()])
            progress = (batch_idx + 1) / total_batches * 100
            self.logger.info(f"   Batch [{batch_idx + 1:5d}/{total_batches}] ({progress:5.1f}%) | {metrics_str}")
            
    def end_epoch(self, epoch: int, total_epochs: int, epoch_metrics: dict, 
                  phase: str = "phase_A") -> bool:
        epoch_time = time.time() - self.epoch_start_time
        self.metrics_history[phase].append(epoch_metrics.copy())
        
        self.logger.info(f"\n   üìä –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ {epoch + 1}:")
        for key, value in epoch_metrics.items():
            if isinstance(value, float):
                self.logger.info(f"      ‚Ä¢ {key}: {value:.6f}")
            else:
                self.logger.info(f"      ‚Ä¢ {key}: {value}")
        self.logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {timedelta(seconds=int(epoch_time))}")
        
        elapsed = time.time() - self.start_time
        avg_epoch_time = elapsed / (epoch + 1)
        remaining = avg_epoch_time * (total_epochs - epoch - 1)
        self.logger.info(f"   ‚è≥ –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ: {timedelta(seconds=int(remaining))}")
        
        if len(self.metrics_history[phase]) >= 2:
            prev = self.metrics_history[phase][-2]
            curr = self.metrics_history[phase][-1]
            
            loss_key = "total" if "total" in curr else "mse_loss"
            if loss_key in prev and loss_key in curr:
                delta = curr[loss_key] - prev[loss_key]
                pct_change = (delta / prev[loss_key]) * 100 if prev[loss_key] != 0 else 0
                direction = "üìâ" if delta < 0 else "üìà" if delta > 0 else "‚û°Ô∏è"
                self.logger.info(f"   {direction} Œî{loss_key}: {delta:+.6f} ({pct_change:+.2f}%)")
        
        current_loss = epoch_metrics.get("total", epoch_metrics.get("mse_loss", float('inf')))
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.patience_counter = 0
        else:
            self.patience_counter += 1
            
        return False
        
    def end_training(self, phase: str):
        total_time = time.time() - self.start_time
        self.logger.info("\n" + "=" * 75)
        self.logger.info(f"‚úÖ {phase} –ó–ê–í–ï–†–®–ï–ù–ê")
        self.logger.info(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {timedelta(seconds=int(total_time))}")
        
        if phase in self.metrics_history and self.metrics_history[phase]:
            first = self.metrics_history[phase][0]
            last = self.metrics_history[phase][-1]
            
            loss_key = "total" if "total" in first else "mse_loss"
            first_loss = first.get(loss_key, 0)
            last_loss = last.get(loss_key, 0)
            
            if first_loss > 0:
                improvement = (first_loss - last_loss) / first_loss * 100
                self.logger.info(f"   üìà –£–ª—É—á—à–µ–Ω–∏–µ loss: {improvement:.2f}%")
            self.logger.info(f"   üìä –ù–∞—á–∞–ª—å–Ω—ã–π loss: {first_loss:.6f}")
            self.logger.info(f"   üìä –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {last_loss:.6f}")
        
        self.logger.info("=" * 75 + "\n")
        
    def log_model_stats(self, model: nn.Module, component_name: str = "–ú–æ–¥–µ–ª—å"):
        self.logger.info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã {component_name}:")
        
        total_params = 0
        trainable_params = 0
        
        for name, module in model.named_children():
            params = sum(p.numel() for p in module.parameters())
            trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            total_params += params
            trainable_params += trainable
            self.logger.info(f"   ‚Ä¢ {name}: {params:,} (trainable: {trainable:,})")
        
        self.logger.info(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        self.logger.info(f"   –í–°–ï–ì–û: {total_params:,} (trainable: {trainable_params:,})")
        self.logger.info(f"   –ü–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏: ~{total_params * 4 / 1024**2:.1f} MB (FP32)")
        
    def log_tensor_stats(self, tensor: torch.Tensor, name: str):
        with torch.no_grad():
            t = tensor.float()
            self.logger.info(f"   üìê {name}:")
            self.logger.info(f"      shape: {list(tensor.shape)}")
            self.logger.info(f"      mean: {t.mean():.6f}, std: {t.std():.6f}")
            self.logger.info(f"      min: {t.min():.6f}, max: {t.max():.6f}")
            
    def save_metrics(self, filepath: str):
        data = {
            "metrics_history": self.metrics_history,
            "best_loss": self.best_loss,
            "timestamp": datetime.now().isoformat()
        }
        try:
            os.makedirs(os.path.dirname(filepath) or ".", exist_ok=True)
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        except OSError as e:
            self.logger.error(f"‚ùå Failed to save metrics to {filepath}: {e}")


# ==============================================================================
# –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–î–ï–õ–ò
# ==============================================================================

class ThoughtEncoder(nn.Module):
    """–≠–Ω–∫–æ–¥–µ—Ä: tokens ‚Üí z —Å Bidirectional LSTM"""
    
    def __init__(self, vocab_size: int, emb_dim: int = 256, z_dim: int = 256, 
                 dropout: float = 0.1):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, emb_dim)
        self.dropout = nn.Dropout(dropout)
        
        self.lstm = nn.LSTM(
            emb_dim, 
            z_dim // 2,
            batch_first=True, 
            bidirectional=True,
            num_layers=1
        )
        
        self.norm = nn.LayerNorm(z_dim)
        self.z_dim = z_dim

    def forward(self, tokens: torch.Tensor) -> torch.Tensor:
        x = self.embed(tokens)
        x = self.dropout(x)
        _, (h, _) = self.lstm(x)
        h = torch.cat([h[0], h[1]], dim=-1)
        z = self.norm(h)
        return z


class VectorQuantizerHybridV4(nn.Module):
    """
    VQ-VAE v4 —Å entropy regularization
    
    –£–ª—É—á—à–µ–Ω–∏—è:
    - Entropy loss –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–æ–≤
    - –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π reset –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–æ–¥–æ–≤
    - –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    """
    
    def __init__(
        self, 
        num_embeddings: int, 
        embedding_dim: int, 
        commitment_cost: float = 0.25,
        decay: float = 0.99,
        epsilon: float = 1e-5,
        temperature: float = 1.0,
        reset_threshold: int = 30,
        entropy_weight: float = 0.1,
        code_reset_noise_scale: float = 0.05
    ):
        super().__init__()
        
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.commitment_cost = commitment_cost
        self.decay = decay
        self.epsilon = epsilon
        self.temperature = temperature
        self.reset_threshold = reset_threshold
        self.entropy_weight = entropy_weight
        self.code_reset_noise_scale = code_reset_noise_scale
        
        # –ö–æ–¥–±—É–∫ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–ª–∏–∂–µ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é z
        self.embedding.weight.data.normal_(0, 0.1)
        
        # EMA –±—É—Ñ–µ—Ä—ã
        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))
        self.register_buffer('ema_w', self.embedding.weight.data.clone())
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.register_buffer('code_usage', torch.zeros(num_embeddings))
        self.register_buffer('code_age', torch.zeros(num_embeddings))
        self.register_buffer('total_count', torch.tensor(0.0))
        self.register_buffer('global_step', torch.tensor(0))

    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
        B, D = z.shape
        
        # –†–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        distances = (
            torch.sum(z**2, dim=1, keepdim=True) + 
            torch.sum(self.embedding.weight**2, dim=1) - 
            2 * torch.matmul(z, self.embedding.weight.t())
        ) / self.temperature
        
        # –í—ã–±–æ—Ä –±–ª–∏–∂–∞–π—à–∏—Ö –∫–æ–¥–æ–≤
        indices = torch.argmin(distances, dim=1)
        
        # –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        quantized = self.embedding(indices)
        
        # ========== LOSS COMPUTATION ==========
        
        # 1. Commitment loss
        commitment_loss = F.mse_loss(z, quantized.detach())
        
        # 2. Codebook loss
        codebook_loss = F.mse_loss(quantized, z.detach())
        
        # 3. ‚úÖ –ù–û–í–û–ï: Entropy regularization
        # –ü–æ–æ—â—Ä—è–µ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤
        entropy_loss = self._compute_entropy_loss(indices)
        
        # –û–±—â–∏–π loss
        loss = codebook_loss + self.commitment_cost * commitment_loss + self.entropy_weight * entropy_loss
        
        # Straight-through estimator
        quantized_st = z + (quantized - z).detach()
        
        # EMA update
        if self.training:
            self._update_ema(z, indices)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = self._compute_stats(indices)
        stats['entropy_loss'] = entropy_loss.item()
        
        return quantized_st, loss, indices, stats
    
    def _compute_entropy_loss(self, indices: torch.Tensor) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç entropy loss –¥–ª—è –ø–æ–æ—â—Ä–µ–Ω–∏—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–æ–≤
        
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è = log(num_embeddings) –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏
        –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º (max_entropy - actual_entropy) / max_entropy
        """
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–¥–∞ –≤ –±–∞—Ç—á–µ
        counts = torch.bincount(indices, minlength=self.num_embeddings).float()
        probs = counts / counts.sum().clamp(min=1)
        
        # Entropy: -sum(p * log(p))
        # –î–æ–±–∞–≤–ª—è–µ–º epsilon –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        log_probs = torch.log(probs + 1e-10)
        entropy = -(probs * log_probs).sum()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏
        max_entropy = math.log(self.num_embeddings)
        
        # Loss = 1 - normalized_entropy (—Ö–æ—Ç–∏–º –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç–Ω—Ç—Ä–æ–ø–∏—é)
        entropy_loss = 1.0 - (entropy / max_entropy) if max_entropy > 0 else torch.tensor(0.0, device=indices.device)
        
        return entropy_loss
    
    def _update_ema(self, z: torch.Tensor, indices: torch.Tensor):
        """EMA –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
        with torch.no_grad():
            self.global_step += 1
            self.total_count += z.size(0)
            
            encodings = F.one_hot(indices, self.num_embeddings).float()
            encodings_sum = encodings.sum(0)
            
            self.ema_cluster_size.mul_(self.decay).add_(encodings_sum, alpha=1 - self.decay)
            
            dw = torch.matmul(encodings.t(), z)
            self.ema_w.mul_(self.decay).add_(dw, alpha=1 - self.decay)
            
            used_codes = indices.unique()
            self.code_usage[used_codes] += 1
            self.code_age += 1
            self.code_age[used_codes] = 0
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π —Å–±—Ä–æ—Å (—á–∞—â–µ —á–µ–º –≤ v3)
            if self.global_step % 50 == 0:
                self._reset_unused_codes(z)
    
    def _reset_unused_codes(self, z: torch.Tensor):
        """–ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å–±—Ä–æ—Å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–æ–¥–æ–≤"""
        unused_mask = self.code_age > self.reset_threshold
        num_unused = unused_mask.sum().item()
        
        if num_unused > 0 and z.size(0) > 0:
            num_to_reset = min(num_unused, z.size(0))
            random_indices = torch.randint(0, z.size(0), (num_to_reset,), device=z.device)
            new_codes = z[random_indices].detach()
            
            # –ë–æ–ª—å—à–µ —à—É–º–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            noise = torch.randn_like(new_codes) * self.code_reset_noise_scale
            new_codes = new_codes + noise
            
            unused_indices = torch.where(unused_mask)[0][:num_to_reset]
            
            self.embedding.weight.data[unused_indices] = new_codes
            self.ema_w[unused_indices] = new_codes
            self.ema_cluster_size[unused_indices] = 1.0
            self.code_age[unused_indices] = 0
            self.code_usage[unused_indices] = 1
    
    def _compute_stats(self, indices: torch.Tensor) -> dict:
        with torch.no_grad():
            unique_in_batch = len(indices.unique())
            total_used = (self.code_usage > 0).sum().item()
            usage_pct = total_used / self.num_embeddings * 100
            
            if self.total_count > 0:
                probs = self.code_usage / (self.total_count + 1e-10)
                probs = probs[probs > 0]
                entropy = -(probs * torch.log(probs + 1e-10)).sum().item()
                max_entropy = math.log(self.num_embeddings)
                normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            else:
                normalized_entropy = 0
            
            return {
                "codebook_usage_%": usage_pct,
                "unique_codes_batch": unique_in_batch,
                "total_used_codes": total_used,
                "codebook_entropy": normalized_entropy,
            }
    
    def get_codebook_usage(self) -> float:
        if self.total_count > 0:
            used = (self.code_usage > 0).sum().item()
            return used / self.num_embeddings * 100
        return 0.0


class ThoughtDecoder(nn.Module):
    """–î–µ–∫–æ–¥–µ—Ä: z + tokens ‚Üí logits"""
    
    def __init__(self, vocab_size: int, emb_dim: int = 256, z_dim: int = 256,
                 dropout: float = 0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.z_dim = z_dim
        
        self.embed = nn.Embedding(vocab_size, emb_dim)
        self.z_proj = nn.Linear(z_dim, emb_dim)
        self.dropout = nn.Dropout(dropout)
        self.lstm = nn.LSTM(emb_dim * 2, emb_dim, batch_first=True)
        self.head = nn.Linear(emb_dim, vocab_size)
        self.head.weight = self.embed.weight  # Weight tying

    def forward(self, z: torch.Tensor, teacher_tokens: torch.Tensor) -> torch.Tensor:
        B, L = teacher_tokens.shape
        
        z_proj = self.z_proj(z)
        z_expanded = z_proj.unsqueeze(1).expand(-1, L, -1)
        
        emb = self.embed(teacher_tokens)
        emb = self.dropout(emb)
        
        lstm_input = torch.cat([emb, z_expanded], dim=-1)
        
        h0 = z_proj.unsqueeze(0)
        c0 = torch.zeros_like(h0)
        
        out, _ = self.lstm(lstm_input, (h0, c0))
        out = self.dropout(out)
        
        logits = self.head(out)
        
        return logits


class ContextualRSSM(nn.Module):
    """
    ‚úÖ –ù–û–í–´–ô: RSSM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º
    
    –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è z_{t+1} —Ç–æ–ª—å–∫–æ –∏–∑ z_t,
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ K —Å–æ—Å—Ç–æ—è–Ω–∏–π: [z_{t-K+1}, ..., z_t] ‚Üí z_{t+1}
    
    –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è —Å–≤—è–∑–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–∞–º –º–µ–∂–¥—É –º—ã—Å–ª—è–º–∏.
    """
    
    def __init__(self, hidden_dim: int, context_window: int = 3, 
                 rssm_hidden: int = 512, dropout: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.context_window = context_window
        self.rssm_hidden = rssm_hidden
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.context_proj = nn.Sequential(
            nn.Linear(hidden_dim * context_window, rssm_hidden),
            nn.LayerNorm(rssm_hidden),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Attention over context (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è)
        self.context_attention = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Softmax(dim=1)
        )
        
        # GRU –¥–ª—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.gru = nn.GRUCell(rssm_hidden, rssm_hidden)
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        self.out_proj = nn.Sequential(
            nn.Linear(rssm_hidden, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # Residual connection weight
        self.residual_weight = nn.Parameter(torch.tensor(0.1))

    def forward(self, z_context: torch.Tensor, 
                hx: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            z_context: [B, K, D] ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ K –ø–æ—Å–ª–µ–¥–Ω–∏—Ö z
            hx: [B, rssm_hidden] ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
            
        Returns:
            z_pred: [B, D] ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ z
        """
        B, K, D = z_context.shape
        
        if hx is None:
            hx = torch.zeros(B, self.rssm_hidden, device=z_context.device)
        
        # Attention-–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        attn_weights = self.context_attention(z_context)  # [B, K, 1]
        weighted_context = (z_context * attn_weights).sum(dim=1)  # [B, D]
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        flat_context = z_context.reshape(B, -1)  # [B, K*D]
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è
        proj = self.context_proj(flat_context)  # [B, rssm_hidden]
        
        # GRU step
        hx_new = self.gru(proj, hx)
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è —Å residual
        z_pred = self.out_proj(hx_new)
        
        # Residual connection –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É z
        z_last = z_context[:, -1, :]
        z_pred = z_pred + self.residual_weight * z_last
        
        return z_pred
    
    def forward_single(self, z_t: torch.Tensor, 
                       hx: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–∑ –æ–¥–Ω–æ–≥–æ z
        """
        # –°–æ–∑–¥–∞—ë–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ z
        z_context = z_t.unsqueeze(1).expand(-1, self.context_window, -1)
        return self.forward(z_context, hx)


class AEONDeltaV4(nn.Module):
    """–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å AEON-Delta v4 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º RSSM"""
    
    def __init__(self, config: AEONConfigV4):
        super().__init__()
        self.config = config
        
        self.tokenizer = None
        if TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {e}")
        
        self.encoder = ThoughtEncoder(
            config.vocab_size, 
            z_dim=config.z_dim,
            dropout=config.dropout_rate
        )
        
        self.vq = VectorQuantizerHybridV4(
            config.vq_num_embeddings, 
            config.vq_embedding_dim,
            commitment_cost=config.vq_commitment_cost,
            decay=config.vq_ema_decay,
            temperature=config.vq_temperature,
            reset_threshold=config.vq_reset_threshold,
            entropy_weight=config.entropy_weight,
            code_reset_noise_scale=config.code_reset_noise_scale
        )
        
        self.decoder = ThoughtDecoder(
            config.vocab_size, 
            z_dim=config.z_dim,
            dropout=config.dropout_rate
        )
        
        # ‚úÖ –ù–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π RSSM
        self.rssm = ContextualRSSM(
            config.hidden_dim, 
            context_window=config.context_window,
            rssm_hidden=config.rssm_hidden_dim,
            dropout=config.dropout_rate
        )
        
        self._init_weights()
        
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.02)
            elif isinstance(module, (nn.LSTM, nn.GRU, nn.GRUCell)):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)

    def encode(self, tokens: torch.Tensor) -> torch.Tensor:
        """Encode token IDs into latent thought vectors.
        
        Args:
            tokens: Input token IDs of shape [B, seq_length].
            
        Returns:
            Latent vectors of shape [B, z_dim].
        """
        return self.encoder(tokens)

    def quantize(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
        """Quantize continuous latent vectors via VQ-VAE.
        
        Args:
            z: Continuous latent vectors of shape [B, z_dim].
            
        Returns:
            Tuple of (quantized, vq_loss, indices, stats).
        """
        return self.vq(z)

    def decode(self, quantized_z: torch.Tensor, teacher_tokens: torch.Tensor) -> torch.Tensor:
        """Decode quantized latent vectors back to token logits.
        
        Args:
            quantized_z: Quantized vectors of shape [B, z_dim].
            teacher_tokens: Teacher-forced token IDs of shape [B, seq_length].
            
        Returns:
            Logits tensor of shape [B, seq_length, vocab_size].
        """
        return self.decoder(quantized_z, teacher_tokens)
    
    def forward(self, tokens: torch.Tensor) -> Dict[str, Any]:
        z = self.encode(tokens)
        quantized, vq_loss, indices, vq_stats = self.quantize(z)
        logits = self.decode(quantized, tokens)
        
        return {
            "z": z,
            "quantized": quantized,
            "vq_loss": vq_loss,
            "indices": indices,
            "logits": logits,
            "vq_stats": vq_stats
        }


# ==============================================================================
# –î–û–ö–£–ú–ï–ù–¢-–û–†–ò–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ô DATASET
# ==============================================================================

class DocumentAwareDataset(Dataset):
    """
    ‚úÖ –ù–û–í–û–ï: Dataset, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç—Ä–æ–∏—Ç z_pairs –¢–û–õ–¨–ö–û –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ RSSM —É—á–∏—Ç—Å—è –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –º—ã—Å–ª–µ–π,
    –∞ –Ω–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–æ—Å–µ–¥—Å—Ç–≤–∞—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    
    def __init__(self, documents: List[List[torch.Tensor]], context_window: int = 3):
        """
        Args:
            documents: List of documents, each is a list of token tensors (chunks).
            context_window: Number of previous z to use as context (must be >= 1).
            
        Raises:
            ValueError: If documents is empty or context_window < 1.
        """
        if not documents:
            raise ValueError("documents list must not be empty")
        if context_window < 1:
            raise ValueError(f"context_window must be >= 1, got {context_window}")
        
        self.context_window = context_window
        self.samples = []  # List of (doc_idx, chunk_indices)
        
        # –°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–µ–º–ø–ª–æ–≤
        for doc_idx, doc_chunks in enumerate(documents):
            num_chunks = len(doc_chunks)
            # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º context_window + 1 —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—ã
            if num_chunks >= context_window + 1:
                for i in range(context_window, num_chunks):
                    # context: [i-context_window, ..., i-1]
                    # target: i
                    self.samples.append((doc_idx, i))
        
        self.documents = documents
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        doc_idx, target_idx = self.samples[idx]
        doc = self.documents[doc_idx]
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_indices = list(range(target_idx - self.context_window, target_idx))
        context_chunks = [doc[i] for i in context_indices]
        target_chunk = doc[target_idx]
        
        return {
            'context': torch.stack(context_chunks),  # [K, seq_len]
            'target': target_chunk  # [seq_len]
        }


# ==============================================================================
# –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø
# ==============================================================================

def tokenize_batch(texts: List[str], tokenizer, max_len: int, 
                   device: torch.device, fallback_vocab_size: int = 50000) -> torch.Tensor:
    """
    Tokenize a batch of text strings into padded token ID tensors.
    
    Args:
        texts: List of text strings to tokenize.
        tokenizer: HuggingFace tokenizer instance, or None for ASCII fallback.
        max_len: Maximum sequence length (texts are truncated/padded to this).
        device: Target device for the output tensor.
        fallback_vocab_size: Vocabulary size for ASCII fallback tokenizer.
        
    Returns:
        Tensor of shape [len(texts), max_len] with token IDs (dtype=torch.long).
    """
    if tokenizer:
        encoded = tokenizer(
            texts, 
            padding='max_length', 
            truncation=True, 
            max_length=max_len, 
            return_tensors='pt'
        )
        return encoded['input_ids'].to(device)
    
    tokenized = []
    for text in texts:
        tokens = [ord(c) % fallback_vocab_size for c in text[:max_len]]
        tokens += [0] * (max_len - len(tokens))
        tokenized.append(tokens)
    return torch.tensor(tokenized, dtype=torch.long, device=device)


def load_documents_from_json(json_path: str, tokenizer, max_len: int, 
                             min_chunks: int = 2, logger=None) -> List[List[torch.Tensor]]:
    """
    Load documents from a JSON-lines file, preserving document structure.
    
    Each line should be a JSON object with one of:
    - {"doc_id": "...", "chunks": ["chunk1 text", "chunk2 text", ...]}
    - {"text": "full document text"} ‚Äî will be split into chunks automatically
    
    Args:
        json_path: Path to the JSON-lines file.
        tokenizer: HuggingFace tokenizer or None (falls back to ASCII tokenization).
        max_len: Maximum token sequence length per chunk.
        min_chunks: Minimum number of chunks per document to include it.
        logger: Optional logger instance.
        
    Returns:
        List of documents, where each document is a list of token tensors.
        
    Raises:
        FileNotFoundError: If json_path does not exist.
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    
    documents = []
    errors = 0
    
    if logger:
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {json_path}...")
    
    with open(json_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                
                if "chunks" in data:
                    # –î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞–Ω–∫–∏
                    chunks = data["chunks"]
                elif "text" in data:
                    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
                    text = data["text"]
                    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º/–∞–±–∑–∞—Ü–∞–º
                    chunks = split_text_into_chunks(text, max_len * 4)  # ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
                else:
                    chunks = [str(data)]
                
                if len(chunks) >= min_chunks:
                    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
                    tokenized_chunks = []
                    for chunk in chunks:
                        if len(chunk.strip()) > 10:
                            tokens = tokenize_batch([chunk], tokenizer, max_len, 
                                                   torch.device('cpu'))[0]
                            tokenized_chunks.append(tokens)
                    
                    if len(tokenized_chunks) >= min_chunks:
                        documents.append(tokenized_chunks)
                        
            except Exception as e:
                errors += 1
                if errors <= 3 and logger:
                    logger.warning(f"   –û—à–∏–±–∫–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
    
    if logger:
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents):,} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        total_chunks = sum(len(d) for d in documents)
        logger.info(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks:,}")
        avg_chunks = total_chunks / len(documents) if documents else 0
        logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {avg_chunks:.1f}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å –æ—à–∏–±–∫–∞–º–∏: {errors}")
    
    return documents


def split_text_into_chunks(text: str, max_chars: int = 256) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º
    sentences = text.replace('\n', ' ').split('. ')
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) + 2 <= max_chars:
            current_chunk += (". " if current_chunk else "") + sentence
        else:
            if current_chunk:
                chunks.append(current_chunk + ".")
            current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk + ".")
    
    return chunks


# ==============================================================================
# LEARNING RATE SCHEDULER
# ==============================================================================

class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_steps: int, total_steps: int,
                 min_lr: float = 1e-7):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        self.current_step = 0
        
    def step(self):
        self.current_step += 1
        lr = self._get_lr()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
            
    def _get_lr(self):
        if self.current_step < self.warmup_steps:
            return self.base_lr * self.current_step / max(1, self.warmup_steps)
        else:
            progress = min(1.0, (self.current_step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps))
            return self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))
    
    def get_lr(self):
        return self.optimizer.param_groups[0]['lr']


# ==============================================================================
# –¢–†–ï–ô–ù–ï–†–´
# ==============================================================================

class SafeThoughtAETrainerV4:
    """–¢—Ä–µ–π–Ω–µ—Ä Phase A: AutoEncoder + VQ v4"""
    
    def __init__(self, model: AEONDeltaV4, config: AEONConfigV4, 
                 monitor: TrainingMonitor, output_dir: str):
        self.model = model
        self.config = config
        self.device = device
        self.monitor = monitor
        self.output_dir = output_dir
        
        self.trainable_params = (
            list(model.encoder.parameters()) + 
            list(model.decoder.parameters()) + 
            list(model.vq.parameters())
        )
        
        self.optimizer = optim.AdamW(
            self.trainable_params, 
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=0,
            label_smoothing=config.label_smoothing
        )
        
        self.use_amp = config.use_amp and AMP_AVAILABLE
        if self.use_amp:
            try:
                self.scaler = GradScaler(device=self.device.type)
            except TypeError:
                self.scaler = GradScaler()
        else:
            self.scaler = None
        
        self.global_step = 0
        self.best_loss = float('inf')
        self.best_model_state = None
        
    def train_step(self, tokens: torch.Tensor) -> Dict[str, float]:
        """Execute a single training step for the autoencoder.
        
        Args:
            tokens: Input token IDs of shape [B, seq_length].
            
        Returns:
            Dictionary with loss values and metrics:
                - total_loss: Combined reconstruction + VQ loss (Tensor).
                - recon_loss: Reconstruction loss (float).
                - vq_loss: Vector quantization loss (float).
                - perplexity: exp(recon_loss) (float).
                - accuracy: Token prediction accuracy percentage (float).
        """
        self.model.train()
        tokens = tokens.to(self.device)
        
        if self.use_amp:
            with autocast(device_type=self.device.type):
                outputs = self._forward_pass(tokens)
        else:
            outputs = self._forward_pass(tokens)
        
        total_loss = outputs['total_loss']
        
        # Detect NaN/Inf loss to prevent corrupted gradient updates
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.warning(
                f"‚ö†Ô∏è NaN/Inf loss detected at step {self.global_step}, skipping backward pass"
            )
            return outputs
        
        if self.use_amp:
            self.scaler.scale(total_loss).backward()
        else:
            total_loss.backward()
        
        return outputs
    
    def _forward_pass(self, tokens: torch.Tensor) -> Dict[str, float]:
        z = self.model.encode(tokens)
        quantized, vq_loss, indices, vq_stats = self.model.quantize(z)
        logits = self.model.decode(quantized, tokens)
        
        recon_loss = self.criterion(
            logits[:, :-1].contiguous().view(-1, self.config.vocab_size), 
            tokens[:, 1:].contiguous().view(-1)
        )
        
        total_loss = recon_loss + self.config.vq_loss_weight * vq_loss
        
        with torch.no_grad():
            perplexity = torch.exp(recon_loss.clamp(max=80)).item()
            pred_tokens = logits[:, :-1].argmax(dim=-1)
            accuracy = (pred_tokens == tokens[:, 1:]).float().mean().item() * 100
        
        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss.item(),
            'vq_loss': vq_loss.item(),
            'perplexity': perplexity,
            'accuracy': accuracy,
            **vq_stats
        }
    
    def _optimizer_step(self):
        if self.use_amp:
            self.scaler.unscale_(self.optimizer)
        
        # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–Ω–∏–∂–µ–Ω–Ω—ã–π grad_clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.trainable_params, 
            self.config.grad_clip_norm  # 0.5 –≤ v4
        )
        
        if self.use_amp:
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            self.optimizer.step()
        
        self.optimizer.zero_grad()
        self.global_step += 1
        
        return grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm

    def fit(self, tokenized_tensor: torch.Tensor, epochs: int = 30, 
            log_every_batch: int = 10):
        
        loader = DataLoader(
            TensorDataset(tokenized_tensor), 
            batch_size=self.config.batch_size, 
            shuffle=True,
            drop_last=True,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        total_batches = len(loader)
        total_steps = epochs * total_batches // self.config.gradient_accumulation_steps
        
        warmup_steps = min(self.config.warmup_steps, total_steps // 10)
        self.scheduler = WarmupCosineScheduler(
            self.optimizer, 
            warmup_steps=warmup_steps,
            total_steps=total_steps,
            min_lr=self.config.min_learning_rate
        )
        
        self.monitor.start_training("Phase A (AutoEncoder + VQ v4)", epochs, len(tokenized_tensor))
        self.monitor.log_model_stats(self.model, "AEON-Delta-v4")
        
        logger.info(f"   ‚úÖ Warmup steps: {warmup_steps}")
        logger.info(f"   ‚úÖ Total steps: {total_steps}")
        logger.info(f"   ‚úÖ Gradient clip: {self.config.grad_clip_norm}")
        logger.info(f"   ‚úÖ Entropy weight: {self.config.entropy_weight}")
        
        self.optimizer.zero_grad()
        
        for epoch in range(epochs):
            self.monitor.start_epoch(epoch, epochs)
            
            epoch_metrics = {
                "recon": 0.0, "vq": 0.0, "total": 0.0, 
                "perplexity": 0.0, "accuracy_%": 0.0, 
                "codebook_%": 0.0, "grad_norm": 0.0
            }
            
            accumulated_loss = 0.0
            num_accumulated = 0
            
            for batch_idx, (batch,) in enumerate(loader):
                outputs = self.train_step(batch)
                step_loss = outputs['total_loss'].item()
                if not (math.isnan(step_loss) or math.isinf(step_loss)):
                    accumulated_loss += step_loss
                    num_accumulated += 1
                
                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                    grad_norm = self._optimizer_step()
                    self.scheduler.step()
                    
                    avg_loss = accumulated_loss / max(num_accumulated, 1)
                    accumulated_loss = 0.0
                    num_accumulated = 0
                    
                    epoch_metrics["total"] += avg_loss
                    if not (math.isnan(outputs['recon_loss']) or math.isinf(outputs['recon_loss'])):
                        epoch_metrics["recon"] += outputs['recon_loss']
                        epoch_metrics["vq"] += outputs['vq_loss']
                        epoch_metrics["perplexity"] += outputs['perplexity']
                        epoch_metrics["accuracy_%"] += outputs['accuracy']
                        epoch_metrics["codebook_%"] += outputs.get('codebook_usage_%', 0)
                    epoch_metrics["grad_norm"] += grad_norm if grad_norm else 0
                
                if batch_idx % log_every_batch == 0:
                    self.monitor.log_batch(batch_idx, total_batches, {
                        "loss": outputs['recon_loss'] + self.config.vq_loss_weight * outputs['vq_loss'],
                        "recon": outputs['recon_loss'],
                        "ppl": outputs['perplexity'],
                        "acc": outputs['accuracy'],
                        "cb%": outputs.get('codebook_usage_%', 0)
                    }, log_every=log_every_batch)
            
            if num_accumulated > 0:
                avg_loss = accumulated_loss / max(num_accumulated, 1)
                epoch_metrics["total"] += avg_loss
                grad_norm = self._optimizer_step()
                self.scheduler.step()
                epoch_metrics["grad_norm"] += grad_norm if grad_norm else 0
            
            num_steps = max(
                (total_batches + self.config.gradient_accumulation_steps - 1) // self.config.gradient_accumulation_steps,
                1
            )
            for key in epoch_metrics:
                epoch_metrics[key] /= num_steps
            
            epoch_metrics["lr"] = self.scheduler.get_lr()
            
            if epoch_metrics["total"] < self.best_loss:
                self.best_loss = epoch_metrics["total"]
                self.best_model_state = copy.deepcopy(self.model.state_dict())
                logger.info(f"   üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π loss: {self.best_loss:.6f}")
            
            self.monitor.end_epoch(epoch, epochs, epoch_metrics, "phase_A")
            
            if (epoch + 1) % self.config.save_every_n_epochs == 0:
                self._save_checkpoint(epoch, epoch_metrics)
        
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            logger.info(f"   ‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å loss={self.best_loss:.6f}")
        
        self.monitor.end_training("phase_A")
    
    def _save_checkpoint(self, epoch: int, metrics: dict):
        try:
            os.makedirs(self.output_dir, exist_ok=True)
            checkpoint_path = os.path.join(
                self.output_dir, 
                f"checkpoint_epoch_{epoch+1}.pt"
            )
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'metrics': metrics,
                'config': asdict(self.config)
            }, checkpoint_path)
            logger.info(f"   üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_path}")
        except OSError as e:
            logger.error(f"   ‚ùå Failed to save checkpoint: {e}")


class ContextualRSSMTrainer:
    """
    ‚úÖ –ù–û–í–´–ô: –¢—Ä–µ–π–Ω–µ—Ä Phase B –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ RSSM
    
    –û–±—É—á–∞–µ—Ç RSSM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å z_{t+1} –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ [z_{t-K+1}, ..., z_t]
    """
    
    def __init__(self, model: AEONDeltaV4, config: AEONConfigV4, 
                 monitor: TrainingMonitor):
        self.model = model
        self.config = config
        self.monitor = monitor
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º encoder, decoder, vq
        for param in model.encoder.parameters():
            param.requires_grad = False
        for param in model.decoder.parameters():
            param.requires_grad = False
        for param in model.vq.parameters():
            param.requires_grad = False
            
        self.trainable_params = list(model.rssm.parameters())
        
        self.optimizer = optim.AdamW(
            self.trainable_params, 
            lr=config.learning_rate * 0.5,
            weight_decay=config.weight_decay
        )
        
        self.best_loss = float('inf')
        self.best_model_state = None
        self.global_step = 0

    def train_step(self, z_context: torch.Tensor, z_target: torch.Tensor) -> Dict[str, float]:
        """
        Single training step for contextual RSSM.
        
        Args:
            z_context: [B, K, D] ‚Äî context from K previous z states
                (B=batch size, K=context window length, D=latent dimension)
            z_target: [B, D] ‚Äî target z_{t+1}
            
        Returns:
            Dictionary with loss and metric values.
        """
        self.model.rssm.train()
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        pred = self.model.rssm(z_context)
        
        # Losses
        mse_loss = F.mse_loss(pred, z_target)
        smooth_l1 = F.smooth_l1_loss(pred, z_target)
        loss = 0.5 * mse_loss + 0.5 * smooth_l1
        
        # Detect NaN/Inf loss to prevent corrupted gradient updates
        if torch.isnan(loss) or torch.isinf(loss):
            logger.warning(
                f"‚ö†Ô∏è NaN/Inf loss detected in RSSM at step {self.global_step}, skipping backward pass"
            )
            return {
                "mse_loss": float('nan'), "smooth_l1": float('nan'),
                "total_loss": float('nan'), "cosine_sim": 0.0,
                "l1_loss": float('nan'), "rel_error": float('nan'),
                "grad_norm": 0.0
            }
        
        self.optimizer.zero_grad()
        loss.backward()
        
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.trainable_params, 
            self.config.grad_clip_norm
        )
        
        self.optimizer.step()
        self.global_step += 1
        
        with torch.no_grad():
            cosine_sim = F.cosine_similarity(pred, z_target, dim=1).mean().item()
            l1_loss = F.l1_loss(pred, z_target).item()
            rel_error = (torch.norm(pred - z_target, dim=1) / (torch.norm(z_target, dim=1) + 1e-8)).mean().item()
        
        return {
            "mse_loss": mse_loss.item(), 
            "smooth_l1": smooth_l1.item(),
            "total_loss": loss.item(),
            "cosine_sim": cosine_sim, 
            "l1_loss": l1_loss,
            "rel_error": rel_error,
            "grad_norm": grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm
        }

    def fit(self, z_sequences: List[torch.Tensor], epochs: int = 10, 
            batch_size: int = 128, log_every_batch: int = 5):
        """
        Args:
            z_sequences: List of [num_chunks, D] tensors, one per document
        """
        # –°–æ–∑–¥–∞—ë–º dataset –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–∫–æ–Ω
        K = self.config.context_window
        
        all_contexts = []
        all_targets = []
        
        for z_seq in z_sequences:
            num_z = z_seq.size(0)
            if num_z >= K + 1:
                for i in range(K, num_z):
                    context = z_seq[i-K:i]  # [K, D]
                    target = z_seq[i]  # [D]
                    all_contexts.append(context)
                    all_targets.append(target)
        
        if len(all_contexts) == 0:
            logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RSSM")
            return
        
        contexts_tensor = torch.stack(all_contexts)  # [N, K, D]
        targets_tensor = torch.stack(all_targets)  # [N, D]
        
        dataset = TensorDataset(contexts_tensor, targets_tensor)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        total_batches = len(loader)
        
        self.monitor.start_training(f"Phase B (Contextual RSSM, K={K})", epochs, len(dataset))
        
        rssm_params = sum(p.numel() for p in self.model.rssm.parameters())
        logger.info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RSSM: {rssm_params:,}")
        logger.info(f"   Context window: {K}")
        logger.info(f"   Training samples: {len(dataset):,}")
        
        for epoch in range(epochs):
            self.monitor.start_epoch(epoch, epochs)
            
            epoch_metrics = {
                "mse_loss": 0.0, "cosine_sim": 0.0, 
                "l1_loss": 0.0, "rel_error": 0.0, "grad_norm": 0.0
            }
            
            for batch_idx, (ctx_batch, tgt_batch) in enumerate(loader):
                ctx_batch = ctx_batch.to(device)
                tgt_batch = tgt_batch.to(device)
                
                metrics = self.train_step(ctx_batch, tgt_batch)
                
                for key in epoch_metrics:
                    if key in metrics and not (math.isnan(metrics[key]) or math.isinf(metrics[key])):
                        epoch_metrics[key] += metrics[key]
                
                if batch_idx % log_every_batch == 0:
                    self.monitor.log_batch(batch_idx, total_batches, {
                        "mse": metrics["mse_loss"],
                        "cos": metrics["cosine_sim"],
                        "rel_err": metrics["rel_error"]
                    }, phase="phase_B", log_every=log_every_batch)
            
            for key in epoch_metrics:
                epoch_metrics[key] /= max(total_batches, 1)
            
            if epoch_metrics["mse_loss"] < self.best_loss:
                self.best_loss = epoch_metrics["mse_loss"]
                self.best_model_state = copy.deepcopy(self.model.rssm.state_dict())
                logger.info(f"   üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π MSE: {self.best_loss:.6f}")
            
            self.monitor.end_epoch(epoch, epochs, epoch_metrics, "phase_B")
        
        # Restore best model
        if self.best_model_state is not None:
            self.model.rssm.load_state_dict(self.best_model_state)
            logger.info(f"   ‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª—É—á—à–∞—è RSSM –º–æ–¥–µ–ª—å —Å MSE={self.best_loss:.6f}")
        
        self.monitor.end_training("phase_B")


# ==============================================================================
# –í–ê–õ–ò–î–ê–¶–ò–Ø
# ==============================================================================

def _validate_component(model_fn, test_input, expected_shape, name, logger):
    """Validate a single model component with shape checking.
    
    Args:
        model_fn: Callable that takes test_input and returns output tensor.
        test_input: Input tensor(s) for the component.
        expected_shape: Expected output shape tuple.
        name: Component name for logging.
        logger: Logger instance.
        
    Returns:
        Tuple of (output_tensor, error_message_or_None).
    """
    try:
        output = model_fn(test_input) if not isinstance(test_input, tuple) else model_fn(*test_input)
        assert output.shape == expected_shape, (
            f"Shape mismatch: expected {expected_shape}, got {output.shape}"
        )
        input_shape = test_input.shape if not isinstance(test_input, tuple) else [t.shape for t in test_input]
        logger.info(f"   ‚úÖ {name}: {input_shape} ‚Üí {output.shape}")
        return output, None
    except Exception as e:
        logger.error(f"   ‚ùå {name}: {e}")
        return None, f"{name}: {e}"


def validate_training_components(model: AEONDeltaV4, config: AEONConfigV4, 
                                  logger: logging.Logger) -> bool:
    """Validate all training components with shape and gradient checks."""
    logger.info("\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è v4...")
    
    issues = []
    test_batch = torch.randint(0, config.vocab_size, (2, config.seq_length), device=device)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Encoder
    try:
        z = model.encode(test_batch)
        assert z.shape == (2, config.z_dim)
        logger.info(f"   ‚úÖ Encoder: {test_batch.shape} ‚Üí {z.shape}")
    except Exception as e:
        issues.append(f"Encoder: {e}")
        logger.error(f"   ‚ùå Encoder: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ VQ
    try:
        quantized, vq_loss, indices, stats = model.quantize(z)
        assert quantized.shape == z.shape
        logger.info(f"   ‚úÖ VectorQuantizer: {z.shape} ‚Üí {quantized.shape}")
        logger.info(f"      entropy_loss: {stats.get('entropy_loss', 'N/A')}")
    except Exception as e:
        issues.append(f"VQ: {e}")
        logger.error(f"   ‚ùå VQ: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Decoder
    try:
        logits = model.decode(quantized, test_batch)
        assert logits.shape == (2, config.seq_length, config.vocab_size)
        logger.info(f"   ‚úÖ Decoder: {quantized.shape} ‚Üí {logits.shape}")
    except Exception as e:
        issues.append(f"Decoder: {e}")
        logger.error(f"   ‚ùå Decoder: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Contextual RSSM
    try:
        K = config.context_window
        z_context = z.unsqueeze(1).expand(-1, K, -1)  # [2, K, D]
        z_pred = model.rssm(z_context)
        assert z_pred.shape == z.shape
        logger.info(f"   ‚úÖ ContextualRSSM: {z_context.shape} ‚Üí {z_pred.shape}")
    except Exception as e:
        issues.append(f"RSSM: {e}")
        logger.error(f"   ‚ùå RSSM: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    model.train()
    model.zero_grad()
    
    z = model.encode(test_batch)
    quantized, vq_loss, _, _ = model.quantize(z)
    logits = model.decode(quantized, test_batch)
    
    recon_loss = F.cross_entropy(logits.view(-1, config.vocab_size), test_batch.view(-1))
    total_loss = recon_loss + vq_loss
    total_loss.backward()
    
    for name, component in [("encoder", model.encoder), ("decoder", model.decoder), ("vq", model.vq)]:
        has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 
                      for p in component.parameters())
        if has_grad:
            logger.info(f"   ‚úÖ {name}: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç")
        else:
            if name == "vq" and hasattr(component, 'embedding'):
                if component.embedding.weight.grad is not None:
                    logger.info(f"   ‚úÖ {name}: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ embedding")
                    continue
            issues.append(f"{name}: –Ω–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
            logger.error(f"   ‚ùå {name}: –Ω–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
    
    model.zero_grad()
    
    if issues:
        logger.error(f"\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(issues)} –ø—Ä–æ–±–ª–µ–º!")
        return False
    
    logger.info("\n‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã v4 –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return True


# ==============================================================================
# –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù v4
# ==============================================================================

def main(
    json_path: str = "combined.json",
    output_dir: str = "processed_v4/",
    epochs_A: int = 30,
    epochs_B: int = 10,
    log_path: str = "training_v4.log",
    resume_from: Optional[str] = None,
    document_aware: bool = True
):
    """Main training pipeline v4.
    
    Args:
        json_path: Path to the input JSON-lines file with documents.
        output_dir: Directory for saving checkpoints, logs, and artifacts.
        epochs_A: Number of epochs for Phase A (AutoEncoder + VQ).
        epochs_B: Number of epochs for Phase B (Contextual RSSM).
        log_path: Path for the training log file.
        resume_from: Optional path to a checkpoint to resume training from.
        document_aware: If True, builds training pairs within document boundaries.
        
    Raises:
        FileNotFoundError: If json_path does not exist.
        ValueError: If epochs_A or epochs_B are non-positive.
    """
    global logger
    
    logger = configure_logger(log_path)
    
    # Validate parameters
    if not os.path.exists(json_path):
        logger.error(f"‚ùå JSON file not found: {json_path}")
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    if epochs_A <= 0:
        raise ValueError(f"epochs_A must be positive, got {epochs_A}")
    if epochs_B <= 0:
        raise ValueError(f"epochs_B must be positive, got {epochs_B}")
    
    monitor = TrainingMonitor(logger, save_dir=os.path.join(output_dir, "checkpoints"))
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    logger.info("üî∑" * 38)
    logger.info("       AEON TRAINING PIPELINE v4.0 - CONNECTED THOUGHTS")
    logger.info("üî∑" * 38)
    logger.info(f"üìÅ –í—Ö–æ–¥–Ω–æ–π JSON: {json_path}")
    logger.info(f"üìÇ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    logger.info(f"üîó Document-aware mode: {document_aware}")

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = None
    if TRANSFORMERS_AVAILABLE:
        try:
            tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v4
    config = AEONConfigV4()
    config.document_aware = document_aware
    
    if tokenizer:
        config.vocab_size = tokenizer.vocab_size
        logger.info(f"üìñ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: bert-base-uncased (vocab_size={config.vocab_size})")

    logger.info(f"\nüìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v4 (–∫–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è):")
    logger.info(f"   ‚Ä¢ grad_clip_norm: {config.grad_clip_norm} (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)")
    logger.info(f"   ‚Ä¢ entropy_weight: {config.entropy_weight} (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–æ–¥–±—É–∫–∞)")
    logger.info(f"   ‚Ä¢ context_window: {config.context_window} (RSSM –∫–æ–Ω—Ç–µ–∫—Å—Ç)")
    logger.info(f"   ‚Ä¢ vq_reset_threshold: {config.vq_reset_threshold} (–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ)")
    logger.info(f"   ‚Ä¢ warmup_steps: {config.warmup_steps} (–ø–ª–∞–≤–Ω–µ–µ)")

    # Seed
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)

    os.makedirs(output_dir, exist_ok=True)

    # ===== –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö =====
    if document_aware:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = load_documents_from_json(
            json_path, tokenizer, config.seq_length,
            min_chunks=config.min_doc_chunks, logger=logger
        )
        
        # –°–æ–∑–¥–∞—ë–º –ø–ª–æ—Å–∫–∏–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è Phase A
        all_tokens = []
        for doc in documents:
            all_tokens.extend(doc)
        tokens = torch.stack(all_tokens).to(device)
        
    else:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–∫–∞–∫ –≤ v3)
        logger.info(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {json_path}...")
        texts = []
        with open(json_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    text = data.get("text", "") if isinstance(data, dict) else str(data)
                    if text and len(text.strip()) > 10:
                        texts.append(text)
                except (json.JSONDecodeError, KeyError, TypeError):
                    pass
        
        tokens = tokenize_batch(texts, tokenizer, config.seq_length, device)
        documents = None
    
    logger.info(f"   –¢–æ–∫–µ–Ω–æ–≤ –¥–ª—è Phase A: {tokens.shape}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã
    try:
        torch.save(tokens.cpu(), os.path.join(output_dir, "tokens.pt"))
    except OSError as e:
        logger.error(f"‚ùå Failed to save tokens: {e}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ v4
    model = AEONDeltaV4(config).to(device)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint
    if resume_from and os.path.exists(resume_from):
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint: {resume_from}")
        try:
            # Try safe loading first
            try:
                checkpoint = torch.load(resume_from, map_location=device, weights_only=True)
            except Exception:
                logger.warning(
                    "‚ö†Ô∏è Loading checkpoint with weights_only=False. "
                    "Only load checkpoints from trusted sources."
                )
                checkpoint = torch.load(resume_from, map_location=device, weights_only=False)
            
            # Validate checkpoint structure
            if not isinstance(checkpoint, dict) or 'model_state_dict' not in checkpoint:
                logger.error(
                    f"‚ùå Checkpoint '{resume_from}' has invalid structure "
                    f"(missing 'model_state_dict' key)."
                )
                return
            
            model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"   ‚úÖ Checkpoint loaded successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to load checkpoint '{resume_from}': {e}")
            return
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    if not validate_training_components(model, config, logger):
        logger.error("‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞!")
        return
    
    # ===== Phase A =====
    logger.info("\n" + "‚ñ∂" * 38)
    logger.info("     PHASE A: AutoEncoder + VQ v4")
    logger.info("‚ñ∂" * 38)
    
    trainer_A = SafeThoughtAETrainerV4(model, config, monitor, output_dir)
    trainer_A.fit(tokens, epochs=epochs_A)

    # Save best loss before releasing Phase A resources
    best_loss_A = trainer_A.best_loss

    # Release Phase A training resources before Phase B
    del trainer_A
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # ===== –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ z_sequences =====
    logger.info("\nüîß –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ z_sequences –¥–ª—è Phase B...")
    model.eval()
    
    with torch.no_grad():
        if document_aware and documents:
            # ‚úÖ –°—Ç—Ä–æ–∏–º z_sequences –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (batch encoding for performance)
            z_sequences = []
            skipped = 0
            
            for doc_idx, doc_chunks in enumerate(tqdm(documents, desc="Encoding documents")):
                if len(doc_chunks) < config.context_window + 1:
                    skipped += 1
                    continue
                
                # Batch encode all chunks in the document at once
                chunks_batch = torch.stack(doc_chunks).to(device)
                z_batch = model.encode(chunks_batch)
                quantized_batch, _, _, _ = model.quantize(z_batch)
                z_seq = quantized_batch.cpu()  # [num_chunks, D]
                z_sequences.append(z_seq)
            
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(z_sequences)} z_sequences (skipped {skipped} docs with < {config.context_window + 1} chunks)")
            total_pairs = sum(max(0, seq.size(0) - config.context_window) for seq in z_sequences)
            logger.info(f"   –í—Å–µ–≥–æ –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {total_pairs:,}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            try:
                torch.save(z_sequences, os.path.join(output_dir, "z_sequences.pt"))
            except OSError as e:
                logger.error(f"‚ùå Failed to save z_sequences: {e}")
            
        else:
            # Fallback: —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ (–≤—Å–µ z –ø–æ–¥—Ä—è–¥)
            z_list = []
            for batch in tqdm(DataLoader(TensorDataset(tokens), batch_size=256), desc="Encoding"):
                z = model.encode(batch[0].to(device))
                quantized, _, _, _ = model.quantize(z)
                z_list.append(quantized.cpu())
            
            z_all = torch.cat(z_list)
            # –°–æ–∑–¥–∞—ë–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π sequence
            z_sequences = [z_all]
            
            try:
                torch.save(z_sequences, os.path.join(output_dir, "z_sequences.pt"))
            except OSError as e:
                logger.error(f"‚ùå Failed to save z_sequences: {e}")

    # Validate z_sequences before Phase B
    if not z_sequences:
        logger.error("‚ùå No z_sequences created ‚Äî cannot run Phase B. "
                      "Check that documents have enough chunks (>= context_window + 1).")
        return

    # ===== Phase B =====
    logger.info("\n" + "‚ñ∂" * 38)
    logger.info("     PHASE B: Contextual RSSM")
    logger.info("‚ñ∂" * 38)
    
    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º sequences –Ω–∞ device
    z_sequences_gpu = [seq.to(device) for seq in z_sequences]
    
    trainer_B = ContextualRSSMTrainer(model, config, monitor)
    trainer_B.fit(z_sequences_gpu, epochs=epochs_B)

    # ===== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ =====
    final_path = os.path.join(output_dir, "aeon_v4_final.pt")
    
    for param in model.parameters():
        param.requires_grad = True
    
    save_dict = {
        'model_state_dict': model.state_dict(),
        'config': asdict(config),
        'metrics_history': monitor.metrics_history,
        'training_info': {
            'epochs_A': epochs_A,
            'epochs_B': epochs_B,
            'final_loss_A': best_loss_A,
            'final_loss_B': trainer_B.best_loss,
            'document_aware': document_aware,
            'timestamp': datetime.now().isoformat(),
            'version': '4.0.0'
        }
    }
    
    try:
        torch.save(save_dict, final_path)
        logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_path}")
    except OSError as e:
        logger.error(f"‚ùå Failed to save final model to {final_path}: {e}")
    monitor.save_metrics(os.path.join(output_dir, "training_metrics_v4.json"))
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
    logger.info("\n" + "üéâ" * 25)
    logger.info("     –û–ë–£–ß–ï–ù–ò–ï v4 –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("üéâ" * 25)
    logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_path}")
    
    logger.info("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê v4:")
    logger.info(f"   Phase A –ª—É—á—à–∏–π loss: {best_loss_A:.6f}")
    logger.info(f"   Phase B –ª—É—á—à–∏–π MSE: {trainer_B.best_loss:.6f}")
    logger.info(f"   Codebook utilization: {model.vq.get_codebook_usage():.2f}%")
    logger.info(f"   Context window: {config.context_window}")
    logger.info(f"   Document-aware: {document_aware}")
    
    logger.info("\nüöÄ –ú–æ–¥–µ–ª—å v4 –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")


# ==============================================================================
# ENTRY POINT
# ==============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="AEON Training Pipeline v4.0 - Connected Thoughts Edition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python train_aeon_v4.py --json_path data.json --epochsA 30 --epochsB 10
  python train_aeon_v4.py --document_aware --json_path structured_data.json
  python train_aeon_v4.py --resume checkpoints/checkpoint_epoch_10.pt
        """
    )
    
    parser.add_argument("--json_path", type=str, default="combined.json")
    parser.add_argument("--output_dir", type=str, default="processed_v4/")
    parser.add_argument("--epochsA", type=int, default=30)
    parser.add_argument("--epochsB", type=int, default=10)
    parser.add_argument("--log", type=str, default="training_v4.log")
    parser.add_argument("--resume", type=str, default=None)
    parser.add_argument("--document_aware", action="store_true", 
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
    
    args = parser.parse_args()
    
    main(
        json_path=args.json_path,
        output_dir=args.output_dir,
        epochs_A=args.epochsA,
        epochs_B=args.epochsB,
        log_path=args.log,
        resume_from=args.resume,
        document_aware=args.document_aware
    )
