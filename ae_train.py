"""
================================================================================
AEON TRAINING PIPELINE v4.0 - CONNECTED THOUGHTS EDITION
================================================================================

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è v4.0:
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π RSSM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º
- ‚úÖ –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (grad_clip —Å–Ω–∏–∂–µ–Ω –¥–æ 0.5)
- ‚úÖ Entropy regularization –¥–ª—è –∫–æ–¥–±—É–∫–∞
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ z_pairs
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π warmup –∏ scheduling

–ê–≤—Ç–æ—Ä: AEON Research Team
–í–µ—Ä—Å–∏—è: 4.0.0
"""

import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import List, Tuple, Dict, Optional, Union, Any
from dataclasses import dataclass, field, asdict
import math
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, **kwargs):
        return iterable
import logging
import os
import time
from datetime import datetime, timedelta
from torch.utils.data import DataLoader, TensorDataset, Dataset
import argparse
import copy
import warnings

warnings.filterwarnings('ignore', category=UserWarning)

__all__ = [
    "AEONConfigV4", "TrainingMonitor",
    "GumbelVectorQuantizer", "VectorQuantizerHybridV4",
    "AEONDeltaV4", "DocumentAwareDataset",
    "WarmupCosineScheduler",
    "SafeThoughtAETrainerV4", "ContextualRSSMTrainer",
    "validate_training_components", "TrainingProvenanceTracker",
    "TrainingConvergenceMonitor", "main",
]

# --- Bridge to aeon_core cognitive architecture ---
# Import core cognitive components for unified coherence verification,
# causal traceability, and meta-cognitive cycle integration during training.
try:
    from aeon_core import (
        CausalProvenanceTracker,
        ConvergenceMonitor,
        SemanticErrorClassifier,
    )
    AEON_CORE_AVAILABLE = True
except ImportError:
    AEON_CORE_AVAILABLE = False

# --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---
try:
    from transformers import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.")

# --- Mixed Precision ---
try:
    from torch.amp import GradScaler, autocast
    AMP_AVAILABLE = torch.cuda.is_available()
except ImportError:
    try:
        from torch.cuda.amp import GradScaler, autocast
        AMP_AVAILABLE = torch.cuda.is_available()
    except ImportError:
        AMP_AVAILABLE = False


# ==============================================================================
# –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
# ==============================================================================

def configure_logger(logfile: Optional[str] = None, level=logging.INFO) -> logging.Logger:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω—Å–æ–ª–∏"""
    logger = logging.getLogger("AEON-Training-v4")
    logger.setLevel(level)
    logger.handlers.clear()
    
    detailed_format = '%(asctime)s | %(levelname)-8s | %(message)s'
    
    sh = logging.StreamHandler()
    sh.setFormatter(logging.Formatter(detailed_format))
    logger.addHandler(sh)
    
    if logfile:
        log_dir = os.path.dirname(logfile) or "."
        os.makedirs(log_dir, exist_ok=True)
        fh = logging.FileHandler(logfile, encoding="utf-8")
        fh.setFormatter(logging.Formatter(detailed_format))
        logger.addHandler(fh)
    
    return logger

logger = configure_logger()


# ==============================================================================
# –£–°–¢–†–û–ô–°–¢–í–û
# ==============================================================================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"üñ•Ô∏è  Device: {device}")
if torch.cuda.is_available():
    logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")
    logger.info(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")


# ==============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø v4.0 ‚Äî –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –î–õ–Ø –°–í–Ø–ó–ê–ù–ù–´–• –ú–´–°–õ–ï–ô
# ==============================================================================

@dataclass
class AEONConfigV4:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v4.0 —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π
    
    –ö–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
    - grad_clip_norm: 0.5 (–±—ã–ª–æ 1.0) ‚Äî —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è
    - context_window: 3 ‚Äî RSSM —É—á–∏—Ç—ã–≤–∞–µ—Ç 3 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è
    - entropy_weight: 0.1 ‚Äî —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–æ–¥–±—É–∫–∞
    - document_aware: True ‚Äî –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–∞—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    """
    
    # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
    z_dim: int = 256
    hidden_dim: int = 256
    vocab_size: int = 30522
    num_pillars: int = 5
    seq_length: int = 64
    
    # VQ-VAE (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
    vq_num_embeddings: int = 2048
    vq_embedding_dim: int = 256
    vq_commitment_cost: float = 0.25
    vq_loss_weight: float = 0.5
    vq_ema_decay: float = 0.99
    vq_temperature: float = 1.0
    vq_reset_threshold: int = 30  # –ë—ã–ª–æ 50, —Ç–µ–ø–µ—Ä—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ
    
    # ‚úÖ –ù–û–í–û–ï: Entropy regularization
    entropy_weight: float = 0.1  # –ü–æ–æ—â—Ä—è–µ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤
    
    # –û–±—É—á–µ–Ω–∏–µ (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
    learning_rate: float = 3e-5
    min_learning_rate: float = 1e-6
    weight_decay: float = 0.01
    grad_clip_norm: float = 0.5  # ‚úÖ –ë—ã–ª–æ 1.0, —Ç–µ–ø–µ—Ä—å —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
    batch_size: int = 16
    gradient_accumulation_steps: int = 2
    
    # Warmup –∏ Scheduling
    warmup_steps: int = 1000  # –ë—ã–ª–æ 500, —Ç–µ–ø–µ—Ä—å –ø–ª–∞–≤–Ω–µ–µ
    warmup_ratio: float = 0.1
    
    # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    dropout_rate: float = 0.1
    label_smoothing: float = 0.1
    
    # ‚úÖ –ù–û–í–û–ï: RSSM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
    context_window: int = 3  # RSSM –≤–∏–¥–∏—Ç 3 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö z
    rssm_hidden_dim: int = 512  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    # ‚úÖ –ù–û–í–û–ï: –î–æ–∫—É–º–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    document_aware: bool = True  # –°—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä—ã —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    min_doc_chunks: int = 2  # –ú–∏–Ω–∏–º—É–º —á–∞–Ω–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    
    # Early Stopping
    early_stopping_patience: int = 5
    min_delta: float = 1e-4
    
    # Checkpointing
    save_every_n_epochs: int = 5
    keep_n_checkpoints: int = 3
    
    # –ü—Ä–æ—á–µ–µ
    seed: int = 42
    use_amp: bool = True
    
    # Noise scale for VQ code reset
    code_reset_noise_scale: float = 0.05

    def __post_init__(self):
        """Validate configuration parameters."""
        if self.z_dim <= 0:
            raise ValueError(f"z_dim must be positive, got {self.z_dim}")
        if self.hidden_dim <= 0:
            raise ValueError(f"hidden_dim must be positive, got {self.hidden_dim}")
        if self.vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {self.vocab_size}")
        if self.seq_length <= 0:
            raise ValueError(f"seq_length must be positive, got {self.seq_length}")
        if self.batch_size <= 0:
            raise ValueError(f"batch_size must be positive, got {self.batch_size}")
        if self.gradient_accumulation_steps <= 0:
            raise ValueError(f"gradient_accumulation_steps must be positive, got {self.gradient_accumulation_steps}")
        if self.learning_rate <= 0:
            raise ValueError(f"learning_rate must be positive, got {self.learning_rate}")
        if self.min_learning_rate <= 0:
            raise ValueError(f"min_learning_rate must be positive, got {self.min_learning_rate}")
        if self.vq_commitment_cost < 0:
            raise ValueError(f"vq_commitment_cost must be non-negative, got {self.vq_commitment_cost}")
        if self.context_window < 1:
            raise ValueError(f"context_window must be >= 1, got {self.context_window}")
        if self.vq_num_embeddings < 2:
            raise ValueError(f"vq_num_embeddings must be >= 2, got {self.vq_num_embeddings}")
        if not (0 < self.vq_ema_decay < 1):
            raise ValueError(f"vq_ema_decay must be in (0, 1), got {self.vq_ema_decay}")
        if self.code_reset_noise_scale < 0:
            raise ValueError(f"code_reset_noise_scale must be non-negative, got {self.code_reset_noise_scale}")
        if not (0 <= self.warmup_ratio <= 1):
            raise ValueError(f"warmup_ratio must be in [0, 1], got {self.warmup_ratio}")
        if not (0 <= self.dropout_rate < 1):
            raise ValueError(f"dropout_rate must be in [0, 1), got {self.dropout_rate}")
        if not (0 <= self.label_smoothing < 1):
            raise ValueError(f"label_smoothing must be in [0, 1), got {self.label_smoothing}")
        if self.vq_temperature <= 0:
            raise ValueError(f"vq_temperature must be positive, got {self.vq_temperature}")
        if self.entropy_weight < 0:
            raise ValueError(f"entropy_weight must be non-negative, got {self.entropy_weight}")
        if self.vq_loss_weight < 0:
            raise ValueError(f"vq_loss_weight must be non-negative, got {self.vq_loss_weight}")
        if self.save_every_n_epochs <= 0:
            raise ValueError(f"save_every_n_epochs must be positive, got {self.save_every_n_epochs}")
        if self.keep_n_checkpoints <= 0:
            raise ValueError(f"keep_n_checkpoints must be positive, got {self.keep_n_checkpoints}")
        if self.min_doc_chunks < 1:
            raise ValueError(f"min_doc_chunks must be >= 1, got {self.min_doc_chunks}")


# ==============================================================================
# –ú–û–ù–ò–¢–û–† –û–ë–£–ß–ï–ù–ò–Ø (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
# ==============================================================================

class TrainingMonitor:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, logger: logging.Logger, save_dir: str = "checkpoints"):
        self.logger = logger
        self.metrics_history = {"phase_A": [], "phase_B": []}
        self.batch_metrics = {"phase_A": [], "phase_B": []}
        self.start_time = None
        self.epoch_start_time = None
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        self.best_loss = float('inf')
        self.patience_counter = 0
        
    def start_training(self, phase: str, total_epochs: int, total_samples: int):
        self.start_time = time.time()
        self.batch_metrics[phase] = []
        self.logger.info("=" * 75)
        self.logger.info(f"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø - {phase}")
        self.logger.info(f"   –í—Å–µ–≥–æ —ç–ø–æ—Ö: {total_epochs}")
        self.logger.info(f"   –í—Å–µ–≥–æ —Å—ç–º–ø–ª–æ–≤: {total_samples:,}")
        self.logger.info(f"   –í—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 75)
        
    def start_epoch(self, epoch: int, total_epochs: int):
        self.epoch_start_time = time.time()
        self.logger.info(f"\n{'‚îÄ' * 60}")
        self.logger.info(f"üìç –≠–ø–æ—Ö–∞ {epoch + 1}/{total_epochs}")
        self.logger.info(f"{'‚îÄ' * 60}")
        
    def log_batch(self, batch_idx: int, total_batches: int, metrics: dict, 
                  phase: str = "phase_A", log_every: int = 10):
        self.batch_metrics[phase].append(metrics.copy())
        
        log_every = max(log_every, 1)
        if batch_idx % log_every == 0 or batch_idx == total_batches - 1:
            metrics_str = " | ".join([f"{k}: {v:.6f}" for k, v in metrics.items()])
            progress = (batch_idx + 1) / max(total_batches, 1) * 100
            self.logger.info(f"   Batch [{batch_idx + 1:5d}/{total_batches}] ({progress:5.1f}%) | {metrics_str}")
            
    def end_epoch(self, epoch: int, total_epochs: int, epoch_metrics: dict, 
                  phase: str = "phase_A") -> bool:
        epoch_time = time.time() - self.epoch_start_time
        self.metrics_history[phase].append(epoch_metrics.copy())
        
        self.logger.info(f"\n   üìä –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ {epoch + 1}:")
        for key, value in epoch_metrics.items():
            if isinstance(value, float):
                self.logger.info(f"      ‚Ä¢ {key}: {value:.6f}")
            else:
                self.logger.info(f"      ‚Ä¢ {key}: {value}")
        self.logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {timedelta(seconds=int(epoch_time))}")
        
        elapsed = time.time() - self.start_time
        avg_epoch_time = elapsed / (epoch + 1)
        remaining = avg_epoch_time * (total_epochs - epoch - 1)
        self.logger.info(f"   ‚è≥ –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ: {timedelta(seconds=int(remaining))}")
        
        if len(self.metrics_history[phase]) >= 2:
            prev = self.metrics_history[phase][-2]
            curr = self.metrics_history[phase][-1]
            
            loss_key = "total" if "total" in curr else "mse_loss"
            if loss_key in prev and loss_key in curr:
                delta = curr[loss_key] - prev[loss_key]
                pct_change = (delta / prev[loss_key]) * 100 if prev[loss_key] != 0 else 0
                direction = "üìâ" if delta < 0 else "üìà" if delta > 0 else "‚û°Ô∏è"
                self.logger.info(f"   {direction} Œî{loss_key}: {delta:+.6f} ({pct_change:+.2f}%)")
        
        current_loss = epoch_metrics.get("total", epoch_metrics.get("mse_loss", float('inf')))
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.patience_counter = 0
        else:
            self.patience_counter += 1
            
        return False
        
    def end_training(self, phase: str):
        total_time = time.time() - self.start_time
        self.logger.info("\n" + "=" * 75)
        self.logger.info(f"‚úÖ {phase} –ó–ê–í–ï–†–®–ï–ù–ê")
        self.logger.info(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {timedelta(seconds=int(total_time))}")
        
        if phase in self.metrics_history and self.metrics_history[phase]:
            first = self.metrics_history[phase][0]
            last = self.metrics_history[phase][-1]
            
            loss_key = "total" if "total" in first else "mse_loss"
            first_loss = first.get(loss_key, 0)
            last_loss = last.get(loss_key, 0)
            
            if first_loss > 0:
                improvement = (first_loss - last_loss) / first_loss * 100
                self.logger.info(f"   üìà –£–ª—É—á—à–µ–Ω–∏–µ loss: {improvement:.2f}%")
            self.logger.info(f"   üìä –ù–∞—á–∞–ª—å–Ω—ã–π loss: {first_loss:.6f}")
            self.logger.info(f"   üìä –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {last_loss:.6f}")
        
        self.logger.info("=" * 75 + "\n")
        
    def log_model_stats(self, model: nn.Module, component_name: str = "–ú–æ–¥–µ–ª—å"):
        self.logger.info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã {component_name}:")
        
        total_params = 0
        trainable_params = 0
        
        for name, module in model.named_children():
            params = sum(p.numel() for p in module.parameters())
            trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            total_params += params
            trainable_params += trainable
            self.logger.info(f"   ‚Ä¢ {name}: {params:,} (trainable: {trainable:,})")
        
        self.logger.info(f"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        self.logger.info(f"   –í–°–ï–ì–û: {total_params:,} (trainable: {trainable_params:,})")
        self.logger.info(f"   –ü–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏: ~{total_params * 4 / 1024**2:.1f} MB (FP32)")
        
    def log_tensor_stats(self, tensor: torch.Tensor, name: str):
        with torch.no_grad():
            t = tensor.float()
            self.logger.info(f"   üìê {name}:")
            self.logger.info(f"      shape: {list(tensor.shape)}")
            self.logger.info(f"      mean: {t.mean():.6f}, std: {t.std():.6f}")
            self.logger.info(f"      min: {t.min():.6f}, max: {t.max():.6f}")
            
    def save_metrics(self, filepath: str):
        data = {
            "metrics_history": self.metrics_history,
            "best_loss": self.best_loss,
            "timestamp": datetime.now().isoformat()
        }
        try:
            os.makedirs(os.path.dirname(filepath) or ".", exist_ok=True)
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        except OSError as e:
            self.logger.error(f"‚ùå Failed to save metrics to {filepath}: {e}")


# ==============================================================================
# –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ú–û–î–ï–õ–ò
# ==============================================================================

class ThoughtEncoder(nn.Module):
    """–≠–Ω–∫–æ–¥–µ—Ä: tokens ‚Üí z —Å Bidirectional LSTM"""
    
    def __init__(self, vocab_size: int, emb_dim: int = 256, z_dim: int = 256, 
                 dropout: float = 0.1):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, emb_dim)
        self.dropout = nn.Dropout(dropout)
        
        self.lstm = nn.LSTM(
            emb_dim, 
            z_dim // 2,
            batch_first=True, 
            bidirectional=True,
            num_layers=1
        )
        
        self.norm = nn.LayerNorm(z_dim)
        self.z_dim = z_dim

    def forward(self, tokens: torch.Tensor) -> torch.Tensor:
        x = self.embed(tokens)
        x = self.dropout(x)
        _, (h, _) = self.lstm(x)
        h = torch.cat([h[0], h[1]], dim=-1)
        z = self.norm(h)
        return z


class GumbelVectorQuantizer(nn.Module):
    """
    Gumbel-Softmax based vector quantizer replacing VQ-VAE.

    Advantages over straight-through VQ-VAE:
    - Fully differentiable (no straight-through hack)
    - Temperature annealing provides smooth transition from soft to hard
    - Less collapsed codes (Gumbel noise prevents mode collapse)

    Reference: Jang et al. 2017 "Categorical Reparameterization with Gumbel-Softmax"
    """

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        commitment_cost: float = 0.25,
        temperature: float = 1.0,
        min_temperature: float = 0.1,
        anneal_rate: float = 1e-5,
        **kwargs,
    ):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.commitment_cost = commitment_cost
        self.min_temperature = min_temperature
        self.anneal_rate = anneal_rate

        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)
        self.embeddings.weight.data.normal_(0, 0.1)

        # Monitoring
        self.register_buffer('code_usage', torch.zeros(num_embeddings))
        self.register_buffer('total_count', torch.tensor(0.0))
        self.register_buffer('global_step', torch.tensor(0))
        self.register_buffer('_temperature', torch.tensor(float(temperature)))

    @property
    def temperature(self) -> float:
        return self._temperature.item()

    @temperature.setter
    def temperature(self, value: float):
        self._temperature.fill_(value)

    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
        """
        Args:
            z: [B, embedding_dim] encoded representation.
        Returns:
            (quantized, loss, indices, stats) matching VectorQuantizerHybridV4 signature.
        """
        # Negative squared distances as logits  [B, num_embeddings]
        logits = -torch.cdist(z.unsqueeze(0), self.embeddings.weight.unsqueeze(0)).squeeze(0)

        if self.training:
            # Gumbel-Softmax: differentiable sampling
            soft_idx = F.gumbel_softmax(logits, tau=self.temperature, hard=False)
            z_q = soft_idx @ self.embeddings.weight  # [B, embedding_dim]
        else:
            # Hard assignment for inference
            soft_idx = F.softmax(logits, dim=-1)
            z_q = soft_idx @ self.embeddings.weight

        # Hard indices for monitoring / stats
        indices = logits.argmax(dim=-1)

        # Loss: commitment + soft entropy regularization
        commitment_loss = F.mse_loss(z, z_q.detach())
        # Entropy regularization to encourage uniform codebook usage
        avg_probs = soft_idx.mean(dim=0)
        entropy = -(avg_probs * torch.log(avg_probs + 1e-10)).sum()
        max_entropy = math.log(self.num_embeddings) if self.num_embeddings > 1 else 1.0
        entropy_loss = 1.0 - entropy / max_entropy

        loss = self.commitment_cost * commitment_loss + 0.1 * entropy_loss

        if self.training:
            self._update_stats(indices)

        stats = self._compute_stats(indices)
        stats['entropy_loss'] = entropy_loss.item()
        stats['temperature'] = self.temperature

        return z_q, loss, indices, stats

    def _update_stats(self, indices: torch.Tensor):
        with torch.no_grad():
            self.global_step += 1
            self.total_count += indices.size(0)
            used = indices.unique()
            self.code_usage[used] += 1
            self.anneal_temperature()

    def anneal_temperature(self):
        """Anneal temperature towards min_temperature. Call after each training step."""
        self.temperature = max(
            self.min_temperature,
            self.temperature * math.exp(-self.anneal_rate),
        )

    def _compute_stats(self, indices: torch.Tensor) -> dict:
        with torch.no_grad():
            unique_in_batch = len(indices.unique())
            total_used = (self.code_usage > 0).sum().item()
            usage_pct = total_used / self.num_embeddings * 100
            if self.total_count > 0:
                probs = self.code_usage / (self.total_count + 1e-10)
                probs = probs[probs > 0]
                entropy = -(probs * torch.log(probs + 1e-10)).sum().item()
                max_ent = math.log(self.num_embeddings) if self.num_embeddings > 1 else 1.0
                normalized_entropy = entropy / max_ent
            else:
                normalized_entropy = 0
            return {
                "codebook_usage_%": usage_pct,
                "unique_codes_batch": unique_in_batch,
                "total_used_codes": total_used,
                "codebook_entropy": normalized_entropy,
            }

    def get_codebook_usage(self) -> float:
        if self.total_count > 0:
            used = (self.code_usage > 0).sum().item()
            return used / self.num_embeddings * 100
        return 0.0


class VectorQuantizerHybridV4(nn.Module):
    """
    VQ-VAE v4 —Å entropy regularization
    
    –£–ª—É—á—à–µ–Ω–∏—è:
    - Entropy loss –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–æ–≤
    - –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π reset –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–æ–¥–æ–≤
    - –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    """
    
    def __init__(
        self, 
        num_embeddings: int, 
        embedding_dim: int, 
        commitment_cost: float = 0.25,
        decay: float = 0.99,
        epsilon: float = 1e-5,
        temperature: float = 1.0,
        reset_threshold: int = 30,
        entropy_weight: float = 0.1,
        code_reset_noise_scale: float = 0.05
    ):
        super().__init__()
        
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.commitment_cost = commitment_cost
        self.decay = decay
        self.epsilon = epsilon
        self.temperature = temperature
        self.reset_threshold = reset_threshold
        self.entropy_weight = entropy_weight
        self.code_reset_noise_scale = code_reset_noise_scale
        
        # –ö–æ–¥–±—É–∫ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–ª–∏–∂–µ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é z
        self.embedding.weight.data.normal_(0, 0.1)
        
        # EMA –±—É—Ñ–µ—Ä—ã
        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))
        self.register_buffer('ema_w', self.embedding.weight.data.clone())
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.register_buffer('code_usage', torch.zeros(num_embeddings))
        self.register_buffer('code_age', torch.zeros(num_embeddings))
        self.register_buffer('total_count', torch.tensor(0.0))
        self.register_buffer('global_step', torch.tensor(0))

    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
        B, D = z.shape
        
        # –†–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        distances = (
            torch.sum(z**2, dim=1, keepdim=True) + 
            torch.sum(self.embedding.weight**2, dim=1) - 
            2 * torch.matmul(z, self.embedding.weight.t())
        ) / max(self.temperature, 1e-8)
        
        # –í—ã–±–æ—Ä –±–ª–∏–∂–∞–π—à–∏—Ö –∫–æ–¥–æ–≤
        indices = torch.argmin(distances, dim=1)
        
        # –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        quantized = self.embedding(indices)
        
        # ========== LOSS COMPUTATION ==========
        
        # 1. Commitment loss
        commitment_loss = F.mse_loss(z, quantized.detach())
        
        # 2. Codebook loss
        codebook_loss = F.mse_loss(quantized, z.detach())
        
        # 3. ‚úÖ –ù–û–í–û–ï: Entropy regularization
        # –ü–æ–æ—â—Ä—è–µ—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤
        # Use soft probabilities from distances for differentiable entropy
        soft_probs = F.softmax(-distances, dim=-1)  # [B, num_embeddings]
        avg_probs = soft_probs.mean(dim=0)  # [num_embeddings]
        entropy = -(avg_probs * torch.log(avg_probs + 1e-10)).sum()
        max_entropy = math.log(self.num_embeddings) if self.num_embeddings > 1 else 1.0
        entropy_loss = 1.0 - entropy / max_entropy
        
        # –û–±—â–∏–π loss
        loss = codebook_loss + self.commitment_cost * commitment_loss + self.entropy_weight * entropy_loss
        
        # Straight-through estimator
        quantized_st = z + (quantized - z).detach()
        
        # EMA update
        if self.training:
            self._update_ema(z, indices)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = self._compute_stats(indices)
        stats['entropy_loss'] = entropy_loss.item()
        
        return quantized_st, loss, indices, stats
    
    def _compute_entropy_loss(self, indices: torch.Tensor) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç entropy loss –¥–ª—è –ø–æ–æ—â—Ä–µ–Ω–∏—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–¥–æ–≤
        
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è = log(num_embeddings) –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏
        –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º (max_entropy - actual_entropy) / max_entropy
        """
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–¥–∞ –≤ –±–∞—Ç—á–µ
        counts = torch.bincount(indices, minlength=self.num_embeddings).float()
        probs = counts / counts.sum().clamp(min=1)
        
        # Entropy: -sum(p * log(p))
        # –î–æ–±–∞–≤–ª—è–µ–º epsilon –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        log_probs = torch.log(probs + 1e-10)
        entropy = -(probs * log_probs).sum()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏
        # Fallback to 1.0 when num_embeddings=1 to avoid division by log(1)=0
        max_entropy = math.log(self.num_embeddings) if self.num_embeddings > 1 else 1.0
        
        # Loss = 1 - normalized_entropy (—Ö–æ—Ç–∏–º –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç–Ω—Ç—Ä–æ–ø–∏—é)
        entropy_loss = 1.0 - (entropy / max_entropy)
        
        return entropy_loss
    
    def _update_ema(self, z: torch.Tensor, indices: torch.Tensor):
        """EMA –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
        with torch.no_grad():
            self.global_step += 1
            self.total_count += z.size(0)
            
            encodings = F.one_hot(indices, self.num_embeddings).float()
            encodings_sum = encodings.sum(0)
            
            self.ema_cluster_size.mul_(self.decay).add_(encodings_sum, alpha=1 - self.decay)
            
            dw = torch.matmul(encodings.t(), z)
            self.ema_w.mul_(self.decay).add_(dw, alpha=1 - self.decay)
            
            used_codes = indices.unique()
            self.code_usage[used_codes] += 1
            self.code_age += 1
            self.code_age[used_codes] = 0
            
            # Update embedding from EMA weights (Laplace-smoothed)
            n = self.ema_cluster_size.sum()
            smoothed = (self.ema_cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n
            self.embedding.weight.data.copy_(
                self.ema_w / smoothed.clamp(min=self.epsilon).unsqueeze(1)
            )
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π —Å–±—Ä–æ—Å (—á–∞—â–µ —á–µ–º –≤ v3)
            if self.global_step % 50 == 0:
                self._reset_unused_codes(z)
    
    def _reset_unused_codes(self, z: torch.Tensor):
        """–ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å–±—Ä–æ—Å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–æ–¥–æ–≤"""
        unused_mask = self.code_age > self.reset_threshold
        num_unused = unused_mask.sum().item()
        
        if num_unused > 0 and z.size(0) > 0:
            num_to_reset = min(num_unused, z.size(0))
            random_indices = torch.randint(0, z.size(0), (num_to_reset,), device=z.device)
            new_codes = z[random_indices].detach()
            
            # –ë–æ–ª—å—à–µ —à—É–º–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            noise = torch.randn_like(new_codes) * self.code_reset_noise_scale
            new_codes = new_codes + noise
            
            unused_indices = torch.where(unused_mask)[0][:num_to_reset]
            
            self.embedding.weight.data[unused_indices] = new_codes
            self.ema_w[unused_indices] = new_codes
            self.ema_cluster_size[unused_indices] = 1.0
            self.code_age[unused_indices] = 0
            self.code_usage[unused_indices] = 1
    
    def _compute_stats(self, indices: torch.Tensor) -> dict:
        with torch.no_grad():
            unique_in_batch = len(indices.unique())
            total_used = (self.code_usage > 0).sum().item()
            usage_pct = total_used / self.num_embeddings * 100
            
            if self.total_count > 0:
                probs = self.code_usage / (self.total_count + 1e-10)
                probs = probs[probs > 0]
                entropy = -(probs * torch.log(probs + 1e-10)).sum().item()
                max_entropy = math.log(self.num_embeddings) if self.num_embeddings > 1 else 1.0
                normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            else:
                normalized_entropy = 0
            
            return {
                "codebook_usage_%": usage_pct,
                "unique_codes_batch": unique_in_batch,
                "total_used_codes": total_used,
                "codebook_entropy": normalized_entropy,
            }
    
    def get_codebook_usage(self) -> float:
        if self.total_count > 0:
            used = (self.code_usage > 0).sum().item()
            return used / self.num_embeddings * 100
        return 0.0


class ThoughtDecoder(nn.Module):
    """–î–µ–∫–æ–¥–µ—Ä: z + tokens ‚Üí logits"""
    
    def __init__(self, vocab_size: int, emb_dim: int = 256, z_dim: int = 256,
                 dropout: float = 0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.z_dim = z_dim
        
        self.embed = nn.Embedding(vocab_size, emb_dim)
        self.z_proj = nn.Linear(z_dim, emb_dim)
        self.dropout = nn.Dropout(dropout)
        self.lstm = nn.LSTM(emb_dim * 2, emb_dim, batch_first=True)
        self.head = nn.Linear(emb_dim, vocab_size)
        self.head.weight = self.embed.weight  # Weight tying

    def forward(self, z: torch.Tensor, teacher_tokens: torch.Tensor) -> torch.Tensor:
        B, L = teacher_tokens.shape
        
        z_proj = self.z_proj(z)
        z_expanded = z_proj.unsqueeze(1).expand(-1, L, -1)
        
        emb = self.embed(teacher_tokens)
        emb = self.dropout(emb)
        
        lstm_input = torch.cat([emb, z_expanded], dim=-1)
        
        h0 = z_proj.unsqueeze(0)
        c0 = torch.zeros_like(h0)
        
        out, _ = self.lstm(lstm_input, (h0, c0))
        out = self.dropout(out)
        
        logits = self.head(out)
        
        return logits


class ContextualRSSM(nn.Module):
    """
    ‚úÖ –ù–û–í–´–ô: RSSM —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º
    
    –í–º–µ—Å—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è z_{t+1} —Ç–æ–ª—å–∫–æ –∏–∑ z_t,
    –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ K —Å–æ—Å—Ç–æ—è–Ω–∏–π: [z_{t-K+1}, ..., z_t] ‚Üí z_{t+1}
    
    –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è —Å–≤—è–∑–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–∞–º –º–µ–∂–¥—É –º—ã—Å–ª—è–º–∏.
    """
    
    def __init__(self, hidden_dim: int, context_window: int = 3, 
                 rssm_hidden: int = 512, dropout: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.context_window = context_window
        self.rssm_hidden = rssm_hidden
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.context_proj = nn.Sequential(
            nn.Linear(hidden_dim * context_window, rssm_hidden),
            nn.LayerNorm(rssm_hidden),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Attention over context (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è)
        self.context_attention = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Softmax(dim=1)
        )
        
        # GRU –¥–ª—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.gru = nn.GRUCell(rssm_hidden, rssm_hidden)
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è
        self.out_proj = nn.Sequential(
            nn.Linear(rssm_hidden, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # Residual connection weight
        self.residual_weight = nn.Parameter(torch.tensor(0.1))

    def forward(self, z_context: torch.Tensor, 
                hx: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            z_context: [B, K, D] ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ K –ø–æ—Å–ª–µ–¥–Ω–∏—Ö z
            hx: [B, rssm_hidden] ‚Äî —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GRU
            
        Returns:
            z_pred: [B, D] ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ z
        """
        B, K, D = z_context.shape
        
        if hx is None:
            hx = torch.zeros(B, self.rssm_hidden, device=z_context.device)
        
        # Attention-–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        attn_weights = self.context_attention(z_context)  # [B, K, 1]
        weighted_context = (z_context * attn_weights).sum(dim=1)  # [B, D]
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        flat_context = z_context.reshape(B, -1)  # [B, K*D]
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è
        proj = self.context_proj(flat_context)  # [B, rssm_hidden]
        
        # GRU step
        hx_new = self.gru(proj, hx)
        
        # –í—ã—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è —Å residual
        z_pred = self.out_proj(hx_new)
        
        # Residual: last z + attention-weighted context
        z_last = z_context[:, -1, :]
        z_pred = z_pred + self.residual_weight * z_last + weighted_context
        
        return z_pred
    
    def forward_single(self, z_t: torch.Tensor, 
                       hx: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–∑ –æ–¥–Ω–æ–≥–æ z
        """
        # –°–æ–∑–¥–∞—ë–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ z
        z_context = z_t.unsqueeze(1).expand(-1, self.context_window, -1)
        return self.forward(z_context, hx)


class AEONDeltaV4(nn.Module):
    """–ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å AEON-Delta v4 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º RSSM"""
    
    def __init__(self, config: AEONConfigV4):
        super().__init__()
        self.config = config
        
        self.tokenizer = None
        if TRANSFORMERS_AVAILABLE:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {e}")
        
        self.encoder = ThoughtEncoder(
            config.vocab_size, 
            z_dim=config.z_dim,
            dropout=config.dropout_rate
        )
        
        self.vq = GumbelVectorQuantizer(
            config.vq_num_embeddings, 
            config.vq_embedding_dim,
            commitment_cost=config.vq_commitment_cost,
            temperature=config.vq_temperature,
        )
        
        self.decoder = ThoughtDecoder(
            config.vocab_size, 
            z_dim=config.z_dim,
            dropout=config.dropout_rate
        )
        
        # ‚úÖ –ù–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π RSSM
        self.rssm = ContextualRSSM(
            config.hidden_dim, 
            context_window=config.context_window,
            rssm_hidden=config.rssm_hidden_dim,
            dropout=config.dropout_rate
        )
        
        self._init_weights()
        
    def _init_weights(self):
        initialized_data_ptrs = set()
        for module in self.modules():
            if isinstance(module, nn.Linear):
                data_ptr = module.weight.data.data_ptr()
                if data_ptr not in initialized_data_ptrs:
                    nn.init.xavier_uniform_(module.weight)
                    initialized_data_ptrs.add(data_ptr)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                data_ptr = module.weight.data.data_ptr()
                if data_ptr not in initialized_data_ptrs:
                    nn.init.normal_(module.weight, std=0.02)
                    initialized_data_ptrs.add(data_ptr)
            elif isinstance(module, (nn.LSTM, nn.GRU, nn.GRUCell)):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.orthogonal_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)

    def encode(self, tokens: torch.Tensor) -> torch.Tensor:
        """Encode token IDs into latent thought vectors.
        
        Args:
            tokens: Input token IDs of shape [B, seq_length].
            
        Returns:
            Latent vectors of shape [B, z_dim].
        """
        return self.encoder(tokens)

    def quantize(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
        """Quantize continuous latent vectors via VQ-VAE.
        
        Args:
            z: Continuous latent vectors of shape [B, z_dim].
            
        Returns:
            Tuple of (quantized, vq_loss, indices, stats).
        """
        return self.vq(z)

    def decode(self, quantized_z: torch.Tensor, teacher_tokens: torch.Tensor) -> torch.Tensor:
        """Decode quantized latent vectors back to token logits.
        
        Args:
            quantized_z: Quantized vectors of shape [B, z_dim].
            teacher_tokens: Teacher-forced token IDs of shape [B, seq_length].
            
        Returns:
            Logits tensor of shape [B, seq_length, vocab_size].
        """
        return self.decoder(quantized_z, teacher_tokens)
    
    def forward(self, tokens: torch.Tensor) -> Dict[str, Any]:
        z = self.encode(tokens)
        quantized, vq_loss, indices, vq_stats = self.quantize(z)
        logits = self.decode(quantized, tokens)
        
        return {
            "z": z,
            "quantized": quantized,
            "vq_loss": vq_loss,
            "indices": indices,
            "logits": logits,
            "vq_stats": vq_stats
        }


# ==============================================================================
# –î–û–ö–£–ú–ï–ù–¢-–û–†–ò–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ô DATASET
# ==============================================================================

class DocumentAwareDataset(Dataset):
    """
    ‚úÖ –ù–û–í–û–ï: Dataset, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç—Ä–æ–∏—Ç z_pairs –¢–û–õ–¨–ö–û –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ RSSM —É—á–∏—Ç—Å—è –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–∞—Ö –º—ã—Å–ª–µ–π,
    –∞ –Ω–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–æ—Å–µ–¥—Å—Ç–≤–∞—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    
    def __init__(self, documents: List[List[torch.Tensor]], context_window: int = 3):
        """
        Args:
            documents: List of documents, each is a list of token tensors (chunks).
            context_window: Number of previous z to use as context (must be >= 1).
            
        Raises:
            ValueError: If documents is empty or context_window < 1.
        """
        if not documents:
            raise ValueError("documents list must not be empty")
        if context_window < 1:
            raise ValueError(f"context_window must be >= 1, got {context_window}")
        
        self.context_window = context_window
        self.samples = []  # List of (doc_idx, chunk_indices)
        
        # –°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–µ–º–ø–ª–æ–≤
        for doc_idx, doc_chunks in enumerate(documents):
            num_chunks = len(doc_chunks)
            # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º context_window + 1 —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—ã
            if num_chunks >= context_window + 1:
                for i in range(context_window, num_chunks):
                    # context: [i-context_window, ..., i-1]
                    # target: i
                    self.samples.append((doc_idx, i))
        
        self.documents = documents
        
        if len(self.samples) == 0:
            logger.warning(
                "DocumentAwareDataset: no valid samples created. "
                "All documents have fewer than context_window + 1 chunks."
            )
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        doc_idx, target_idx = self.samples[idx]
        doc = self.documents[doc_idx]
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_indices = list(range(target_idx - self.context_window, target_idx))
        context_chunks = [doc[i] for i in context_indices]
        target_chunk = doc[target_idx]
        
        return {
            'context': torch.stack(context_chunks),  # [K, seq_len]
            'target': target_chunk  # [seq_len]
        }


# ==============================================================================
# –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø
# ==============================================================================

def tokenize_batch(texts: List[str], tokenizer, max_len: int, 
                   device: torch.device, fallback_vocab_size: int = 50000) -> torch.Tensor:
    """
    Tokenize a batch of text strings into padded token ID tensors.
    
    Args:
        texts: List of text strings to tokenize.
        tokenizer: HuggingFace tokenizer instance, or None for ASCII fallback.
        max_len: Maximum sequence length (texts are truncated/padded to this).
        device: Target device for the output tensor.
        fallback_vocab_size: Vocabulary size for ASCII fallback tokenizer.
        
    Returns:
        Tensor of shape [len(texts), max_len] with token IDs (dtype=torch.long).
    """
    if tokenizer:
        encoded = tokenizer(
            texts, 
            padding='max_length', 
            truncation=True, 
            max_length=max_len, 
            return_tensors='pt'
        )
        return encoded['input_ids'].to(device)
    
    tokenized = []
    for text in texts:
        tokens = [ord(c) % fallback_vocab_size for c in text[:max_len]]
        tokens += [0] * (max_len - len(tokens))
        tokenized.append(tokens)
    if not tokenized:
        return torch.zeros((0, max_len), dtype=torch.long, device=device)
    return torch.tensor(tokenized, dtype=torch.long, device=device)


def load_documents_from_json(json_path: str, tokenizer, max_len: int, 
                             min_chunks: int = 2, logger=None) -> List[List[torch.Tensor]]:
    """
    Load documents from a JSON-lines file, preserving document structure.
    
    Each line should be a JSON object with one of:
    - {"doc_id": "...", "chunks": ["chunk1 text", "chunk2 text", ...]}
    - {"text": "full document text"} ‚Äî will be split into chunks automatically
    
    Args:
        json_path: Path to the JSON-lines file.
        tokenizer: HuggingFace tokenizer or None (falls back to ASCII tokenization).
        max_len: Maximum token sequence length per chunk.
        min_chunks: Minimum number of chunks per document to include it.
        logger: Optional logger instance.
        
    Returns:
        List of documents, where each document is a list of token tensors.
        
    Raises:
        FileNotFoundError: If json_path does not exist.
    """
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    
    documents = []
    errors = 0
    
    if logger:
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {json_path}...")
    
    with open(json_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                
                if "chunks" in data:
                    # –î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞–Ω–∫–∏
                    chunks = data["chunks"]
                elif "text" in data:
                    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
                    text = data["text"]
                    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º/–∞–±–∑–∞—Ü–∞–º
                    chunks = split_text_into_chunks(text, max_len * 4)  # ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
                else:
                    chunks = [str(data)]
                
                if len(chunks) >= min_chunks:
                    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
                    tokenized_chunks = []
                    for chunk in chunks:
                        if len(chunk.strip()) > 10:
                            tokens = tokenize_batch([chunk], tokenizer, max_len, 
                                                   torch.device('cpu'))[0]
                            tokenized_chunks.append(tokens)
                    
                    if len(tokenized_chunks) >= min_chunks:
                        documents.append(tokenized_chunks)
                        
            except Exception as e:
                errors += 1
                if errors <= 3 and logger:
                    logger.warning(f"   –û—à–∏–±–∫–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
                elif errors == 4 and logger:
                    logger.warning("   ... (suppressing further per-line error messages)")
    
    if logger:
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents):,} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        total_chunks = sum(len(d) for d in documents)
        logger.info(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks:,}")
        avg_chunks = total_chunks / len(documents) if documents else 0
        logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤/–¥–æ–∫—É–º–µ–Ω—Ç: {avg_chunks:.1f}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å –æ—à–∏–±–∫–∞–º–∏: {errors}")
    
    return documents


def split_text_into_chunks(text: str, max_chars: int = 256) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
    if not isinstance(text, str) or not text.strip():
        return []
    
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º
    sentences = text.replace('\n', ' ').split('. ')
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) + 2 <= max_chars:
            current_chunk += (". " if current_chunk else "") + sentence
        else:
            if current_chunk:
                chunks.append(current_chunk + ".")
            current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk + ".")
    
    return chunks


# ==============================================================================
# LEARNING RATE SCHEDULER
# ==============================================================================

class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_steps: int, total_steps: int,
                 min_lr: float = 1e-7):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        self.current_step = 0
        
    def step(self):
        self.current_step += 1
        lr = self._get_lr()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
            
    def _get_lr(self):
        if self.current_step < self.warmup_steps:
            return self.base_lr * self.current_step / max(1, self.warmup_steps)
        else:
            progress = min(1.0, (self.current_step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps))
            return self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))
    
    def get_lr(self):
        return self.optimizer.param_groups[0]['lr']


# ==============================================================================
# –¢–†–ï–ô–ù–ï–†–´
# ==============================================================================

class SafeThoughtAETrainerV4:
    """–¢—Ä–µ–π–Ω–µ—Ä Phase A: AutoEncoder + VQ v4"""
    
    def __init__(self, model: AEONDeltaV4, config: AEONConfigV4, 
                 monitor: TrainingMonitor, output_dir: str):
        self.model = model
        self.config = config
        self.device = next(model.parameters()).device
        self.monitor = monitor
        self.output_dir = output_dir
        
        self.trainable_params = (
            list(model.encoder.parameters()) + 
            list(model.decoder.parameters()) + 
            list(model.vq.parameters())
        )
        
        self.optimizer = optim.AdamW(
            self.trainable_params, 
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        self.criterion = nn.CrossEntropyLoss(
            ignore_index=0,
            label_smoothing=config.label_smoothing
        )
        
        self.use_amp = config.use_amp and AMP_AVAILABLE
        if self.use_amp:
            try:
                self.scaler = GradScaler(device=self.device.type)
            except TypeError:
                self.scaler = GradScaler()
        else:
            self.scaler = None
        
        self.global_step = 0
        self.best_loss = float('inf')
        self.best_model_state = None
        
        # Bridge to aeon_core convergence monitoring ‚Äî tracks loss
        # trajectory across epochs to detect divergence/stagnation
        # and trigger adaptive training adjustments.
        self.convergence_monitor = TrainingConvergenceMonitor(
            threshold=1e-5, window_size=10,
        )
        self.provenance = TrainingProvenanceTracker()
        # Error classifier for semantic error categorization when
        # aeon_core is available; provides richer diagnostics than
        # raw exception strings.
        self._error_classifier = (
            SemanticErrorClassifier() if AEON_CORE_AVAILABLE else None
        )
        
    def train_step(self, tokens: torch.Tensor) -> Dict[str, Any]:
        """Execute a single training step for the autoencoder.
        
        Args:
            tokens: Input token IDs of shape [B, seq_length].
            
        Returns:
            Dictionary with loss values and metrics:
                - total_loss: Combined reconstruction + VQ loss (Tensor).
                - recon_loss: Reconstruction loss (float).
                - vq_loss: Vector quantization loss (float).
                - perplexity: exp(recon_loss) (float).
                - accuracy: Token prediction accuracy percentage (float).
        """
        self.model.train()
        tokens = tokens.to(self.device)
        
        if self.use_amp:
            with autocast(device_type=self.device.type):
                outputs = self._forward_pass(tokens)
        else:
            outputs = self._forward_pass(tokens)
        
        total_loss = outputs['total_loss']
        
        # Detect NaN/Inf loss to prevent corrupted gradient updates.
        # When aeon_core is available, classify the error semantically
        # so root-cause analysis can trace it to a specific component
        # via the provenance tracker.
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            _error_detail = "NaN/Inf loss"
            if self._error_classifier is not None:
                try:
                    _err_cls, _err_detail = self._error_classifier.classify(
                        RuntimeError("NaN/Inf loss in training step")
                    )
                    _error_detail = f"{_err_cls}: {_err_detail}"
                except Exception:
                    pass
            # Identify the dominant provenance module at the point of
            # failure so the error is traceable to its root cause.
            _provenance = outputs.get('provenance', {})
            _dominant = None
            _contributions = _provenance.get('contributions', {})
            if _contributions:
                _dominant = max(_contributions, key=_contributions.get)
            logger.warning(
                f"‚ö†Ô∏è {_error_detail} at step {self.global_step}"
                f" (dominant_module={_dominant}), skipping backward pass"
            )
            return outputs
        
        if self.use_amp:
            self.scaler.scale(total_loss).backward()
        else:
            total_loss.backward()
        
        return outputs
    
    def _forward_pass(self, tokens: torch.Tensor) -> Dict[str, Any]:
        # Track per-component provenance so training errors can be
        # traced back to their originating component.
        self.provenance.reset()

        z = self.model.encode(tokens)
        # Record encoder output as both before/after VQ input
        self.provenance.record_before("vq", z)
        quantized, vq_loss, indices, vq_stats = self.model.quantize(z)
        self.provenance.record_after("vq", quantized)

        self.provenance.record_before("decoder", quantized)
        logits = self.model.decode(quantized, tokens)
        # Record decoder delta using mean-pooled output (seq √ó vocab ‚Üí z_dim)
        self.provenance.record_after("decoder", logits.mean(dim=(1, 2)).unsqueeze(-1).expand_as(quantized))
        
        recon_loss = self.criterion(
            logits[:, :-1].contiguous().view(-1, self.config.vocab_size), 
            tokens[:, 1:].contiguous().view(-1)
        )
        
        total_loss = recon_loss + self.config.vq_loss_weight * vq_loss
        
        with torch.no_grad():
            perplexity = torch.exp(recon_loss.clamp(max=80)).item()
            pred_tokens = logits[:, :-1].argmax(dim=-1)
            accuracy = (pred_tokens == tokens[:, 1:]).float().mean().item() * 100
        
        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss.item(),
            'vq_loss': vq_loss.item(),
            'perplexity': perplexity,
            'accuracy': accuracy,
            'provenance': self.provenance.compute_attribution(),
            **vq_stats
        }
    
    def _optimizer_step(self):
        if self.use_amp:
            self.scaler.unscale_(self.optimizer)
        
        # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–Ω–∏–∂–µ–Ω–Ω—ã–π grad_clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.trainable_params, 
            self.config.grad_clip_norm  # 0.5 –≤ v4
        )
        
        if self.use_amp:
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            self.optimizer.step()
        
        self.optimizer.zero_grad()
        self.global_step += 1
        
        return grad_norm.item() if torch.is_tensor(grad_norm) else float(grad_norm)

    def fit(self, tokenized_tensor: torch.Tensor, epochs: int = 30, 
            log_every_batch: int = 10):
        
        loader = DataLoader(
            TensorDataset(tokenized_tensor), 
            batch_size=self.config.batch_size, 
            shuffle=True,
            drop_last=True,
            num_workers=0,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        total_batches = len(loader)
        total_steps = epochs * total_batches // self.config.gradient_accumulation_steps
        
        warmup_steps = min(self.config.warmup_steps, total_steps // 10)
        self.scheduler = WarmupCosineScheduler(
            self.optimizer, 
            warmup_steps=warmup_steps,
            total_steps=total_steps,
            min_lr=self.config.min_learning_rate
        )
        
        self.monitor.start_training("Phase A (AutoEncoder + VQ v4)", epochs, len(tokenized_tensor))
        self.monitor.log_model_stats(self.model, "AEON-Delta-v4")
        
        logger.info(f"   ‚úÖ Warmup steps: {warmup_steps}")
        logger.info(f"   ‚úÖ Total steps: {total_steps}")
        logger.info(f"   ‚úÖ Gradient clip: {self.config.grad_clip_norm}")
        logger.info(f"   ‚úÖ Entropy weight: {self.config.entropy_weight}")
        
        self.optimizer.zero_grad()
        
        for epoch in range(epochs):
            self.monitor.start_epoch(epoch, epochs)
            
            epoch_metrics = {
                "recon": 0.0, "vq": 0.0, "total": 0.0, 
                "perplexity": 0.0, "accuracy_%": 0.0, 
                "codebook_%": 0.0, "grad_norm": 0.0
            }
            
            accumulated_loss = 0.0
            num_accumulated = 0
            
            for batch_idx, (batch,) in enumerate(loader):
                outputs = self.train_step(batch)
                step_loss = outputs['total_loss'].item()
                if not (math.isnan(step_loss) or math.isinf(step_loss)):
                    accumulated_loss += step_loss
                    num_accumulated += 1
                
                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                    if num_accumulated > 0:
                        grad_norm = self._optimizer_step()
                        self.scheduler.step()
                    else:
                        self.optimizer.zero_grad()
                        grad_norm = 0.0
                    
                    avg_loss = accumulated_loss / max(num_accumulated, 1)
                    accumulated_loss = 0.0
                    num_accumulated = 0
                    
                    epoch_metrics["total"] += avg_loss
                    if not (math.isnan(outputs['recon_loss']) or math.isinf(outputs['recon_loss'])):
                        epoch_metrics["recon"] += outputs['recon_loss']
                        epoch_metrics["vq"] += outputs['vq_loss']
                        epoch_metrics["perplexity"] += outputs['perplexity']
                        epoch_metrics["accuracy_%"] += outputs['accuracy']
                        epoch_metrics["codebook_%"] += outputs.get('codebook_usage_%', 0)
                    epoch_metrics["grad_norm"] += grad_norm if (grad_norm is not None and math.isfinite(grad_norm)) else 0
                
                if batch_idx % log_every_batch == 0:
                    self.monitor.log_batch(batch_idx, total_batches, {
                        "loss": outputs['recon_loss'] + self.config.vq_loss_weight * outputs['vq_loss'],
                        "recon": outputs['recon_loss'],
                        "ppl": outputs['perplexity'],
                        "acc": outputs['accuracy'],
                        "cb%": outputs.get('codebook_usage_%', 0)
                    }, log_every=log_every_batch)
            
            if num_accumulated > 0:
                avg_loss = accumulated_loss / max(num_accumulated, 1)
                epoch_metrics["total"] += avg_loss
                if not (math.isnan(outputs['recon_loss']) or math.isinf(outputs['recon_loss'])):
                    epoch_metrics["recon"] += outputs['recon_loss']
                    epoch_metrics["vq"] += outputs['vq_loss']
                    epoch_metrics["perplexity"] += outputs['perplexity']
                    epoch_metrics["accuracy_%"] += outputs['accuracy']
                    epoch_metrics["codebook_%"] += outputs.get('codebook_usage_%', 0)
                grad_norm = self._optimizer_step()
                self.scheduler.step()
                epoch_metrics["grad_norm"] += grad_norm if (grad_norm is not None and math.isfinite(grad_norm)) else 0
            
            num_steps = max(
                (total_batches + self.config.gradient_accumulation_steps - 1) // self.config.gradient_accumulation_steps,
                1
            )
            for key in epoch_metrics:
                epoch_metrics[key] /= num_steps
            
            epoch_metrics["lr"] = self.scheduler.get_lr()
            
            # Convergence monitoring ‚Äî track loss trajectory across
            # epochs to detect divergence, stagnation, or convergence.
            # This bridges the training loop to aeon_core's
            # ConvergenceMonitor pattern, ensuring that training
            # dynamics feed back into adaptive behavior.
            convergence_verdict = self.convergence_monitor.update(
                epoch_metrics["total"]
            )
            epoch_metrics["convergence_status"] = convergence_verdict["status"]
            if convergence_verdict["status"] == "diverging":
                logger.warning(
                    f"   ‚ö†Ô∏è Convergence monitor: DIVERGING "
                    f"(trend={convergence_verdict['trend']:.6f}). "
                    f"Recommendation: {convergence_verdict['recommendation']}"
                )
            elif convergence_verdict["status"] == "stagnating":
                logger.info(
                    f"   ‚ÑπÔ∏è Convergence monitor: stagnating "
                    f"(trend={convergence_verdict['trend']:.6f})"
                )
            
            if epoch_metrics["total"] < self.best_loss:
                self.best_loss = epoch_metrics["total"]
                self.best_model_state = copy.deepcopy(self.model.state_dict())
                logger.info(f"   üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π loss: {self.best_loss:.6f}")
            
            self.monitor.end_epoch(epoch, epochs, epoch_metrics, "phase_A")
            
            if (epoch + 1) % self.config.save_every_n_epochs == 0:
                self._save_checkpoint(epoch, epoch_metrics)
        
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            logger.info(f"   ‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å loss={self.best_loss:.6f}")
        
        self.monitor.end_training("phase_A")
    
    def _save_checkpoint(self, epoch: int, metrics: dict):
        try:
            os.makedirs(self.output_dir, exist_ok=True)
            checkpoint_path = os.path.join(
                self.output_dir, 
                f"checkpoint_epoch_{epoch+1}.pt"
            )
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'metrics': metrics,
                'config': asdict(self.config)
            }, checkpoint_path)
            logger.info(f"   üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_path}")
        except OSError as e:
            logger.error(f"   ‚ùå Failed to save checkpoint: {e}")


class ContextualRSSMTrainer:
    """
    ‚úÖ –ù–û–í–´–ô: –¢—Ä–µ–π–Ω–µ—Ä Phase B –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ RSSM
    
    –û–±—É—á–∞–µ—Ç RSSM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å z_{t+1} –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ [z_{t-K+1}, ..., z_t]
    """
    
    def __init__(self, model: AEONDeltaV4, config: AEONConfigV4, 
                 monitor: TrainingMonitor):
        self.model = model
        self.config = config
        self.monitor = monitor
        self.device = next(model.parameters()).device
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º encoder, decoder, vq
        for param in model.encoder.parameters():
            param.requires_grad = False
        for param in model.decoder.parameters():
            param.requires_grad = False
        for param in model.vq.parameters():
            param.requires_grad = False
            
        self.trainable_params = list(model.rssm.parameters())
        
        self.optimizer = optim.AdamW(
            self.trainable_params, 
            lr=config.learning_rate * 0.5,
            weight_decay=config.weight_decay
        )
        
        self.best_loss = float('inf')
        self.best_model_state = None
        self.global_step = 0
        # Convergence monitor for Phase B loss trajectory
        self.convergence_monitor = TrainingConvergenceMonitor(
            threshold=1e-5, window_size=10,
        )

    def train_step(self, z_context: torch.Tensor, z_target: torch.Tensor) -> Dict[str, float]:
        """
        Single training step for contextual RSSM.
        
        Args:
            z_context: [B, K, D] ‚Äî context from K previous z states
                (B=batch size, K=context window length, D=latent dimension)
            z_target: [B, D] ‚Äî target z_{t+1}
            
        Returns:
            Dictionary with loss and metric values.
        """
        self.model.rssm.train()
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        pred = self.model.rssm(z_context)
        
        # Losses
        mse_loss = F.mse_loss(pred, z_target)
        smooth_l1 = F.smooth_l1_loss(pred, z_target)
        loss = 0.5 * mse_loss + 0.5 * smooth_l1
        
        # Detect NaN/Inf loss to prevent corrupted gradient updates
        if torch.isnan(loss) or torch.isinf(loss):
            logger.warning(
                f"‚ö†Ô∏è NaN/Inf loss detected in RSSM at step {self.global_step}, skipping backward pass"
            )
            return {
                "mse_loss": float('nan'), "smooth_l1": float('nan'),
                "total_loss": float('nan'), "cosine_sim": 0.0,
                "l1_loss": float('nan'), "rel_error": float('nan'),
                "grad_norm": 0.0
            }
        
        self.optimizer.zero_grad()
        loss.backward()
        
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.trainable_params, 
            self.config.grad_clip_norm
        )
        
        self.optimizer.step()
        self.global_step += 1
        
        with torch.no_grad():
            cosine_sim = F.cosine_similarity(pred, z_target, dim=1).mean().item()
            l1_loss = F.l1_loss(pred, z_target).item()
            rel_error = (torch.norm(pred - z_target, dim=1) / (torch.norm(z_target, dim=1) + 1e-8)).clamp(max=1e4).mean().item()
        
        return {
            "mse_loss": mse_loss.item(), 
            "smooth_l1": smooth_l1.item(),
            "total_loss": loss.item(),
            "cosine_sim": cosine_sim, 
            "l1_loss": l1_loss,
            "rel_error": rel_error,
            "grad_norm": grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm
        }

    def fit(self, z_sequences: List[torch.Tensor], epochs: int = 10, 
            batch_size: int = 128, log_every_batch: int = 5):
        """
        Args:
            z_sequences: List of [num_chunks, D] tensors, one per document
        """
        # –°–æ–∑–¥–∞—ë–º dataset –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–∫–æ–Ω
        K = self.config.context_window
        
        all_contexts = []
        all_targets = []
        
        for z_seq in z_sequences:
            num_z = z_seq.size(0)
            if num_z >= K + 1:
                for i in range(K, num_z):
                    context = z_seq[i-K:i]  # [K, D]
                    target = z_seq[i]  # [D]
                    all_contexts.append(context)
                    all_targets.append(target)
        
        if len(all_contexts) == 0:
            logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RSSM")
            return
        
        contexts_tensor = torch.stack(all_contexts)  # [N, K, D]
        targets_tensor = torch.stack(all_targets)  # [N, D]
        
        dataset = TensorDataset(contexts_tensor, targets_tensor)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)
        total_batches = len(loader)
        
        self.monitor.start_training(f"Phase B (Contextual RSSM, K={K})", epochs, len(dataset))
        
        rssm_params = sum(p.numel() for p in self.model.rssm.parameters())
        logger.info(f"üì¶ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RSSM: {rssm_params:,}")
        logger.info(f"   Context window: {K}")
        logger.info(f"   Training samples: {len(dataset):,}")
        
        for epoch in range(epochs):
            self.monitor.start_epoch(epoch, epochs)
            
            epoch_metrics = {
                "mse_loss": 0.0, "cosine_sim": 0.0, 
                "l1_loss": 0.0, "rel_error": 0.0, "grad_norm": 0.0
            }
            valid_batches = 0
            
            for batch_idx, (ctx_batch, tgt_batch) in enumerate(loader):
                ctx_batch = ctx_batch.to(self.device)
                tgt_batch = tgt_batch.to(self.device)
                
                metrics = self.train_step(ctx_batch, tgt_batch)
                
                batch_valid = False
                for key in epoch_metrics:
                    if key in metrics and not (math.isnan(metrics[key]) or math.isinf(metrics[key])):
                        epoch_metrics[key] += metrics[key]
                        batch_valid = True
                if batch_valid:
                    valid_batches += 1
                
                if batch_idx % log_every_batch == 0:
                    self.monitor.log_batch(batch_idx, total_batches, {
                        "mse": metrics["mse_loss"],
                        "cos": metrics["cosine_sim"],
                        "rel_err": metrics["rel_error"]
                    }, phase="phase_B", log_every=log_every_batch)
            
            for key in epoch_metrics:
                epoch_metrics[key] /= max(valid_batches, 1)
            
            # Convergence monitoring for Phase B
            convergence_verdict = self.convergence_monitor.update(
                epoch_metrics["mse_loss"]
            )
            epoch_metrics["convergence_status"] = convergence_verdict["status"]
            if convergence_verdict["status"] == "diverging":
                logger.warning(
                    f"   ‚ö†Ô∏è Phase B convergence: DIVERGING "
                    f"(trend={convergence_verdict['trend']:.6f})"
                )
            
            if epoch_metrics["mse_loss"] < self.best_loss:
                self.best_loss = epoch_metrics["mse_loss"]
                self.best_model_state = copy.deepcopy(self.model.rssm.state_dict())
                logger.info(f"   üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π MSE: {self.best_loss:.6f}")
            
            self.monitor.end_epoch(epoch, epochs, epoch_metrics, "phase_B")
        
        # Restore best model
        if self.best_model_state is not None:
            self.model.rssm.load_state_dict(self.best_model_state)
            logger.info(f"   ‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª—É—á—à–∞—è RSSM –º–æ–¥–µ–ª—å —Å MSE={self.best_loss:.6f}")
        
        self.monitor.end_training("phase_B")


# ==============================================================================
# TRAINING‚ÄìCORE BRIDGE: PROVENANCE & CONVERGENCE
# ==============================================================================

# Threshold above which a single module's provenance contribution
# triggers a dominance warning during validation.
_PROVENANCE_DOMINANCE_WARNING_THRESHOLD = 0.9

class TrainingProvenanceTracker:
    """Lightweight provenance tracker for the training pipeline.

    Bridges ae_train's training loop to aeon_core's
    :class:`CausalProvenanceTracker` pattern by recording per-component
    L2 deltas during validation and training steps.  When aeon_core is
    available, delegates to the real tracker; otherwise uses a minimal
    standalone implementation so the training pipeline always has
    provenance data regardless of import availability.

    This closes the architectural gap where training errors could not be
    traced back to their originating component.
    """

    def __init__(self):
        if AEON_CORE_AVAILABLE:
            self._tracker = CausalProvenanceTracker()
        else:
            self._tracker = None
        # Standalone fallback storage
        self._deltas: Dict[str, float] = {}
        self._order: list = []
        self._snapshots: Dict[str, torch.Tensor] = {}

    def reset(self):
        """Clear all recorded snapshots for a new pass."""
        if self._tracker is not None:
            self._tracker.reset()
        self._deltas.clear()
        self._order.clear()
        self._snapshots.clear()

    def record_before(self, name: str, state: torch.Tensor):
        if self._tracker is not None:
            self._tracker.record_before(name, state)
        self._snapshots[name] = state.detach().clone()
        if name not in self._order:
            self._order.append(name)

    def record_after(self, name: str, state: torch.Tensor):
        if self._tracker is not None:
            self._tracker.record_after(name, state)
        if name in self._snapshots:
            before = self._snapshots[name]
            # Handle shape mismatches by truncating to smaller size
            min_size = min(state.shape[-1], before.shape[-1])
            self._deltas[name] = (
                state.detach()[..., :min_size] - before[..., :min_size]
            ).norm().item()

    def compute_attribution(self) -> Dict[str, Any]:
        """Return per-component attribution dict."""
        if self._tracker is not None:
            return self._tracker.compute_attribution()
        total = sum(self._deltas.values()) + 1e-10
        contributions = {k: v / total for k, v in self._deltas.items()}
        return {
            'contributions': contributions,
            'deltas': dict(self._deltas),
            'order': list(self._order),
        }


class TrainingConvergenceMonitor:
    """Monitors training loss convergence across epochs.

    Bridges ae_train's training loop to aeon_core's
    :class:`ConvergenceMonitor` pattern by tracking a sliding window of
    loss values and detecting divergence or stagnation.

    When aeon_core is available, delegates to the real monitor;
    otherwise uses a minimal standalone implementation.

    Attributes:
        status: One of ``'warmup'``, ``'converging'``, ``'converged'``,
            ``'diverging'``, or ``'stagnating'``.
    """

    _WARMUP_SIZE = 5
    _DIVERGENCE_RATIO = 1.5
    _STAGNATION_THRESHOLD = 1e-6

    def __init__(self, threshold: float = 1e-5, window_size: int = 10):
        self._threshold = threshold
        self._window_size = window_size
        self._history: list = []
        self.status: str = 'warmup'
        if AEON_CORE_AVAILABLE:
            self._core_monitor = ConvergenceMonitor(threshold=threshold)
        else:
            self._core_monitor = None

    def update(self, loss_value: float) -> Dict[str, Any]:
        """Record a loss value and return convergence verdict.

        Args:
            loss_value: Scalar training loss for the current epoch/step.

        Returns:
            Dict with ``status``, ``loss_value``, ``trend`` (float),
            and ``recommendation`` (str).
        """
        if not math.isfinite(loss_value):
            self.status = 'diverging'
            return {
                'status': 'diverging',
                'loss_value': loss_value,
                'trend': float('inf'),
                'recommendation': 'reduce_lr_or_rollback',
            }

        self._history.append(loss_value)
        if len(self._history) > self._window_size:
            self._history = self._history[-self._window_size:]

        if len(self._history) < self._WARMUP_SIZE:
            self.status = 'warmup'
            return {
                'status': 'warmup',
                'loss_value': loss_value,
                'trend': 0.0,
                'recommendation': 'continue',
            }

        recent = self._history[-self._WARMUP_SIZE:]
        trend = recent[-1] - recent[0]

        if recent[-1] > self._DIVERGENCE_RATIO * min(recent):
            self.status = 'diverging'
            recommendation = 'reduce_lr_or_rollback'
        elif abs(trend) < self._STAGNATION_THRESHOLD:
            self.status = 'stagnating'
            recommendation = 'increase_lr_or_augment'
        elif trend < 0:
            if abs(trend) < self._threshold:
                self.status = 'converged'
                recommendation = 'continue'
            else:
                self.status = 'converging'
                recommendation = 'continue'
        else:
            self.status = 'diverging'
            recommendation = 'reduce_lr_or_rollback'

        # Feed into aeon_core monitor if available
        if self._core_monitor is not None:
            self._core_monitor.check(abs(trend))

        return {
            'status': self.status,
            'loss_value': loss_value,
            'trend': trend,
            'recommendation': recommendation,
        }


# ==============================================================================
# –í–ê–õ–ò–î–ê–¶–ò–Ø
# ==============================================================================

def _validate_component(model_fn, test_input, expected_shape, name, logger):
    """Validate a single model component with shape checking.
    
    Args:
        model_fn: Callable that takes test_input and returns output tensor.
        test_input: Input tensor(s) for the component.
        expected_shape: Expected output shape tuple.
        name: Component name for logging.
        logger: Logger instance.
        
    Returns:
        Tuple of (output_tensor, error_message_or_None).
    """
    try:
        output = model_fn(test_input) if not isinstance(test_input, tuple) else model_fn(*test_input)
        assert output.shape == expected_shape, (
            f"Shape mismatch: expected {expected_shape}, got {output.shape}"
        )
        input_shape = test_input.shape if not isinstance(test_input, tuple) else [t.shape for t in test_input]
        logger.info(f"   ‚úÖ {name}: {input_shape} ‚Üí {output.shape}")
        return output, None
    except Exception as e:
        logger.error(f"   ‚ùå {name}: {e}")
        return None, f"{name}: {e}"


def validate_training_components(model: AEONDeltaV4, config: AEONConfigV4, 
                                  logger: logging.Logger) -> bool:
    """Validate all training components with shape and gradient checks."""
    logger.info("\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è v4...")
    
    issues = []
    model_device = next(model.parameters()).device
    test_batch = torch.randint(0, config.vocab_size, (2, config.seq_length), device=model_device)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Encoder
    try:
        z = model.encode(test_batch)
        assert z.shape == (2, config.z_dim)
        logger.info(f"   ‚úÖ Encoder: {test_batch.shape} ‚Üí {z.shape}")
    except Exception as e:
        issues.append(f"Encoder: {e}")
        logger.error(f"   ‚ùå Encoder: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ VQ
    try:
        quantized, vq_loss, indices, stats = model.quantize(z)
        assert quantized.shape == z.shape
        logger.info(f"   ‚úÖ VectorQuantizer: {z.shape} ‚Üí {quantized.shape}")
        logger.info(f"      entropy_loss: {stats.get('entropy_loss', 'N/A')}")
    except Exception as e:
        issues.append(f"VQ: {e}")
        logger.error(f"   ‚ùå VQ: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Decoder
    try:
        logits = model.decode(quantized, test_batch)
        assert logits.shape == (2, config.seq_length, config.vocab_size)
        logger.info(f"   ‚úÖ Decoder: {quantized.shape} ‚Üí {logits.shape}")
    except Exception as e:
        issues.append(f"Decoder: {e}")
        logger.error(f"   ‚ùå Decoder: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Contextual RSSM
    try:
        K = config.context_window
        z_context = z.unsqueeze(1).expand(-1, K, -1)  # [2, K, D]
        z_pred = model.rssm(z_context)
        assert z_pred.shape == z.shape
        logger.info(f"   ‚úÖ ContextualRSSM: {z_context.shape} ‚Üí {z_pred.shape}")
    except Exception as e:
        issues.append(f"RSSM: {e}")
        logger.error(f"   ‚ùå RSSM: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    model.train()
    model.zero_grad()
    
    z = model.encode(test_batch)
    quantized, vq_loss, _, _ = model.quantize(z)
    logits = model.decode(quantized, test_batch)
    
    recon_loss = F.cross_entropy(logits.view(-1, config.vocab_size), test_batch.view(-1))
    total_loss = recon_loss + vq_loss
    total_loss.backward()
    
    for name, component in [("encoder", model.encoder), ("decoder", model.decoder), ("vq", model.vq)]:
        has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 
                      for p in component.parameters())
        if has_grad:
            logger.info(f"   ‚úÖ {name}: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç")
        else:
            embed_attr = getattr(component, 'embedding', None) or getattr(component, 'embeddings', None)
            if name == "vq" and embed_attr is not None:
                if embed_attr.weight.grad is not None:
                    logger.info(f"   ‚úÖ {name}: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ embedding")
                    continue
            issues.append(f"{name}: –Ω–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
            logger.error(f"   ‚ùå {name}: –Ω–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
    
    model.zero_grad()
    
    # ===== COGNITIVE COHERENCE VERIFICATION =====
    # Cross-validate component outputs using provenance tracking from
    # aeon_core's CausalProvenanceTracker pattern.  This ensures that
    # each component verifies and reinforces the others ‚Äî a core
    # requirement for a unified cognitive system.
    logger.info("\nüîç Cognitive coherence verification...")
    provenance = TrainingProvenanceTracker()
    try:
        model.eval()
        with torch.no_grad():
            # Track provenance through the full pipeline
            provenance.reset()

            z_val = model.encode(test_batch)

            provenance.record_before("vq", z_val)
            q_val, _, _, _ = model.quantize(z_val)
            provenance.record_after("vq", q_val)

            provenance.record_before("rssm", q_val)
            K = config.context_window
            z_ctx = q_val.unsqueeze(1).expand(-1, K, -1)
            z_pred_val = model.rssm(z_ctx)
            provenance.record_after("rssm", z_pred_val)

            attribution = provenance.compute_attribution()
            contributions = attribution.get('contributions', {})
            if contributions:
                _max_contrib = max(contributions.values())
                _dominant = max(contributions, key=contributions.get)
                logger.info(
                    f"   ‚úÖ Provenance: dominant_module={_dominant} "
                    f"({_max_contrib:.1%}), "
                    f"modules={list(contributions.keys())}"
                )
                # Warn if a single module dominates ‚Äî indicates
                # an architectural imbalance where one component
                # overwhelms the others.
                if _max_contrib > _PROVENANCE_DOMINANCE_WARNING_THRESHOLD:
                    logger.warning(
                        f"   ‚ö†Ô∏è Module '{_dominant}' dominates "
                        f"provenance ({_max_contrib:.1%} > "
                        f"{_PROVENANCE_DOMINANCE_WARNING_THRESHOLD:.0%}). "
                        f"Consider rebalancing component contributions."
                    )

            # Cross-module coherence: verify that encoder output and
            # VQ output are semantically aligned (cosine similarity).
            # A threshold of -0.5 catches severe misalignment while
            # allowing normal VQ quantization shifts that may reduce
            # cosine similarity from 1.0.
            _COHERENCE_THRESHOLD = -0.5
            cos_sim = F.cosine_similarity(z_val, q_val, dim=-1).mean().item()
            if cos_sim > _COHERENCE_THRESHOLD:
                logger.info(
                    f"   ‚úÖ Encoder‚ÜîVQ coherence: cos_sim={cos_sim:.4f}"
                )
            else:
                logger.warning(
                    f"   ‚ö†Ô∏è Low encoder‚ÜîVQ coherence: cos_sim={cos_sim:.4f}"
                )

            # Verify RSSM prediction is finite and reasonably close to input
            if torch.isfinite(z_pred_val).all():
                rssm_cos = F.cosine_similarity(
                    q_val, z_pred_val, dim=-1
                ).mean().item()
                logger.info(
                    f"   ‚úÖ VQ‚ÜîRSSM coherence: cos_sim={rssm_cos:.4f}"
                )
            else:
                issues.append("RSSM: non-finite predictions")
                logger.error("   ‚ùå RSSM produces non-finite values")

    except Exception as coherence_err:
        logger.warning(
            f"   ‚ö†Ô∏è Coherence verification failed (non-fatal): {coherence_err}"
        )
    
    if issues:
        logger.error(f"\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(issues)} –ø—Ä–æ–±–ª–µ–º!")
        return False
    
    logger.info("\n‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã v4 –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
    return True


# ==============================================================================
# –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù v4
# ==============================================================================

def main(
    json_path: str = "combined.json",
    output_dir: str = "processed_v4/",
    epochs_A: int = 30,
    epochs_B: int = 10,
    log_path: str = "training_v4.log",
    resume_from: Optional[str] = None,
    document_aware: bool = True
):
    """Main training pipeline v4.
    
    Args:
        json_path: Path to the input JSON-lines file with documents.
        output_dir: Directory for saving checkpoints, logs, and artifacts.
        epochs_A: Number of epochs for Phase A (AutoEncoder + VQ).
        epochs_B: Number of epochs for Phase B (Contextual RSSM).
        log_path: Path for the training log file.
        resume_from: Optional path to a checkpoint to resume training from.
        document_aware: If True, builds training pairs within document boundaries.
        
    Raises:
        FileNotFoundError: If json_path does not exist.
        ValueError: If epochs_A or epochs_B are non-positive.
    """
    global logger
    
    logger = configure_logger(log_path)
    
    # Validate parameters
    if not os.path.exists(json_path):
        logger.error(f"‚ùå JSON file not found: {json_path}")
        raise FileNotFoundError(f"JSON file not found: {json_path}")
    if epochs_A <= 0:
        raise ValueError(f"epochs_A must be positive, got {epochs_A}")
    if epochs_B <= 0:
        raise ValueError(f"epochs_B must be positive, got {epochs_B}")
    
    monitor = TrainingMonitor(logger, save_dir=os.path.join(output_dir, "checkpoints"))
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    logger.info("üî∑" * 38)
    logger.info("       AEON TRAINING PIPELINE v4.0 - CONNECTED THOUGHTS")
    logger.info("üî∑" * 38)
    logger.info(f"üìÅ –í—Ö–æ–¥–Ω–æ–π JSON: {json_path}")
    logger.info(f"üìÇ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    logger.info(f"üîó Document-aware mode: {document_aware}")

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = None
    if TRANSFORMERS_AVAILABLE:
        try:
            tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v4
    config = AEONConfigV4()
    config.document_aware = document_aware
    
    if tokenizer:
        config.vocab_size = tokenizer.vocab_size
        logger.info(f"üìñ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: bert-base-uncased (vocab_size={config.vocab_size})")

    logger.info(f"\nüìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è v4 (–∫–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è):")
    logger.info(f"   ‚Ä¢ grad_clip_norm: {config.grad_clip_norm} (—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)")
    logger.info(f"   ‚Ä¢ entropy_weight: {config.entropy_weight} (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∫–æ–¥–±—É–∫–∞)")
    logger.info(f"   ‚Ä¢ context_window: {config.context_window} (RSSM –∫–æ–Ω—Ç–µ–∫—Å—Ç)")
    logger.info(f"   ‚Ä¢ vq_reset_threshold: {config.vq_reset_threshold} (–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ)")
    logger.info(f"   ‚Ä¢ warmup_steps: {config.warmup_steps} (–ø–ª–∞–≤–Ω–µ–µ)")

    # Seed
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)

    os.makedirs(output_dir, exist_ok=True)

    # ===== –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö =====
    if document_aware:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = load_documents_from_json(
            json_path, tokenizer, config.seq_length,
            min_chunks=config.min_doc_chunks, logger=logger
        )
        
        # –°–æ–∑–¥–∞—ë–º –ø–ª–æ—Å–∫–∏–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è Phase A
        all_tokens = []
        for doc in documents:
            all_tokens.extend(doc)
        
        if not all_tokens:
            logger.error("‚ùå No token chunks found in documents ‚Äî cannot train.")
            return
        
        tokens = torch.stack(all_tokens).to(device)
        
    else:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–∫–∞–∫ –≤ v3)
        logger.info(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {json_path}...")
        texts = []
        with open(json_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    text = data.get("text", "") if isinstance(data, dict) else str(data)
                    if text and len(text.strip()) > 10:
                        texts.append(text)
                except (json.JSONDecodeError, KeyError, TypeError) as e:
                    logger.debug(f"Skipping malformed line: {e}")
        
        tokens = tokenize_batch(texts, tokenizer, config.seq_length, device)
        documents = None
    
    if tokens.numel() == 0:
        logger.error("‚ùå No valid training data found ‚Äî tokens tensor is empty.")
        return
    
    logger.info(f"   –¢–æ–∫–µ–Ω–æ–≤ –¥–ª—è Phase A: {tokens.shape}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã
    try:
        torch.save(tokens.cpu(), os.path.join(output_dir, "tokens.pt"))
    except OSError as e:
        logger.error(f"‚ùå Failed to save tokens: {e}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ v4
    model = AEONDeltaV4(config).to(device)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint
    if resume_from and os.path.exists(resume_from):
        logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint: {resume_from}")
        try:
            # Try safe loading first
            try:
                checkpoint = torch.load(resume_from, map_location=device, weights_only=True)
            except (RuntimeError, TypeError):
                logger.warning(
                    "‚ö†Ô∏è Loading checkpoint with weights_only=False. "
                    "Only load checkpoints from trusted sources."
                )
                checkpoint = torch.load(resume_from, map_location=device, weights_only=False)
            
            # Validate checkpoint structure
            if not isinstance(checkpoint, dict) or 'model_state_dict' not in checkpoint:
                logger.error(
                    f"‚ùå Checkpoint '{resume_from}' has invalid structure "
                    f"(missing 'model_state_dict' key)."
                )
                return
            
            model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"   ‚úÖ Checkpoint loaded successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to load checkpoint '{resume_from}': {e}")
            return
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    if not validate_training_components(model, config, logger):
        logger.error("‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞!")
        return
    
    # ===== Phase A =====
    logger.info("\n" + "‚ñ∂" * 38)
    logger.info("     PHASE A: AutoEncoder + VQ v4")
    logger.info("‚ñ∂" * 38)
    
    trainer_A = SafeThoughtAETrainerV4(model, config, monitor, output_dir)
    trainer_A.fit(tokens, epochs=epochs_A)

    # Save best loss before releasing Phase A resources
    best_loss_A = trainer_A.best_loss

    # Release Phase A training resources before Phase B
    del trainer_A
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # ===== –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ z_sequences =====
    logger.info("\nüîß –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ z_sequences –¥–ª—è Phase B...")
    model.eval()
    
    with torch.no_grad():
        if document_aware and documents:
            # ‚úÖ –°—Ç—Ä–æ–∏–º z_sequences –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (batch encoding for performance)
            z_sequences = []
            skipped = 0
            
            for doc_idx, doc_chunks in enumerate(tqdm(documents, desc="Encoding documents")):
                if len(doc_chunks) < config.context_window + 1:
                    skipped += 1
                    continue
                
                # Batch encode all chunks in the document at once
                chunks_batch = torch.stack(doc_chunks).to(device)
                z_batch = model.encode(chunks_batch)
                quantized_batch, _, _, _ = model.quantize(z_batch)
                z_seq = quantized_batch.cpu()  # [num_chunks, D]
                z_sequences.append(z_seq)
            
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(z_sequences)} z_sequences (skipped {skipped} docs with < {config.context_window + 1} chunks)")
            total_pairs = sum(max(0, seq.size(0) - config.context_window) for seq in z_sequences)
            logger.info(f"   –í—Å–µ–≥–æ –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {total_pairs:,}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            try:
                torch.save(z_sequences, os.path.join(output_dir, "z_sequences.pt"))
            except OSError as e:
                logger.error(f"‚ùå Failed to save z_sequences: {e}")
            
        else:
            # Fallback: —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ (–≤—Å–µ z –ø–æ–¥—Ä—è–¥)
            z_list = []
            for batch in tqdm(DataLoader(TensorDataset(tokens), batch_size=256), desc="Encoding"):
                z = model.encode(batch[0].to(device))
                quantized, _, _, _ = model.quantize(z)
                z_list.append(quantized.cpu())
            
            z_all = torch.cat(z_list)
            # –°–æ–∑–¥–∞—ë–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π sequence
            z_sequences = [z_all]
            
            try:
                torch.save(z_sequences, os.path.join(output_dir, "z_sequences.pt"))
            except OSError as e:
                logger.error(f"‚ùå Failed to save z_sequences: {e}")

    # Validate z_sequences before Phase B
    if not z_sequences:
        logger.error("‚ùå No z_sequences created ‚Äî cannot run Phase B. "
                      "Check that documents have enough chunks (>= context_window + 1).")
        return

    # ===== Phase B =====
    logger.info("\n" + "‚ñ∂" * 38)
    logger.info("     PHASE B: Contextual RSSM")
    logger.info("‚ñ∂" * 38)
    
    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º sequences –Ω–∞ device
    z_sequences_gpu = [seq.to(device) for seq in z_sequences]
    
    trainer_B = ContextualRSSMTrainer(model, config, monitor)
    trainer_B.fit(z_sequences_gpu, epochs=epochs_B)

    # ===== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ =====
    final_path = os.path.join(output_dir, "aeon_v4_final.pt")
    
    for param in model.parameters():
        param.requires_grad = True
    
    save_dict = {
        'model_state_dict': model.state_dict(),
        'config': asdict(config),
        'metrics_history': monitor.metrics_history,
        'training_info': {
            'epochs_A': epochs_A,
            'epochs_B': epochs_B,
            'final_loss_A': best_loss_A,
            'final_loss_B': trainer_B.best_loss,
            'document_aware': document_aware,
            'timestamp': datetime.now().isoformat(),
            'version': '4.0.0'
        }
    }
    
    try:
        torch.save(save_dict, final_path)
        logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_path}")
    except OSError as e:
        logger.error(f"‚ùå Failed to save final model to {final_path}: {e}")
    monitor.save_metrics(os.path.join(output_dir, "training_metrics_v4.json"))
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
    logger.info("\n" + "üéâ" * 25)
    logger.info("     –û–ë–£–ß–ï–ù–ò–ï v4 –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("üéâ" * 25)
    logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_path}")
    
    logger.info("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê v4:")
    logger.info(f"   Phase A –ª—É—á—à–∏–π loss: {best_loss_A:.6f}")
    logger.info(f"   Phase B –ª—É—á—à–∏–π MSE: {trainer_B.best_loss:.6f}")
    logger.info(f"   Codebook utilization: {model.vq.get_codebook_usage():.2f}%")
    logger.info(f"   Context window: {config.context_window}")
    logger.info(f"   Document-aware: {document_aware}")
    
    logger.info("\nüöÄ –ú–æ–¥–µ–ª—å v4 –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")


# ==============================================================================
# ENTRY POINT
# ==============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="AEON Training Pipeline v4.0 - Connected Thoughts Edition",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python train_aeon_v4.py --json_path data.json --epochsA 30 --epochsB 10
  python train_aeon_v4.py --document_aware --json_path structured_data.json
  python train_aeon_v4.py --resume checkpoints/checkpoint_epoch_10.pt
        """
    )
    
    parser.add_argument("--json_path", type=str, default="combined.json")
    parser.add_argument("--output_dir", type=str, default="processed_v4/")
    parser.add_argument("--epochsA", type=int, default=30)
    parser.add_argument("--epochsB", type=int, default=10)
    parser.add_argument("--log", type=str, default="training_v4.log")
    parser.add_argument("--resume", type=str, default=None)
    parser.add_argument("--document_aware", action="store_true", 
                        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
    
    args = parser.parse_args()
    
    main(
        json_path=args.json_path,
        output_dir=args.output_dir,
        epochs_A=args.epochsA,
        epochs_B=args.epochsB,
        log_path=args.log,
        resume_from=args.resume,
        document_aware=args.document_aware
    )
